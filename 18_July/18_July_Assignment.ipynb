{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa6a266-6f2d-4e28-a816-da474cf3279c",
   "metadata": {},
   "source": [
    "## TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9ea2d-3bc8-49ac-b235-f5dc1686153b",
   "metadata": {},
   "source": [
    "### 1.Describe the purpose and benefits of pooling in CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489dcb32-c4b3-41b0-a52b-9659bc3331ac",
   "metadata": {},
   "source": [
    "Pooling, also known as subsampling or downsampling, is a crucial operation in Convolutional Neural Networks (CNNs) that helps reduce the spatial dimensions of feature maps while retaining important information. The primary purpose of pooling is to reduce the computational complexity of the network, control overfitting, and increase the network's translation invariance.\n",
    "\n",
    "The main benefits of pooling in CNNs are as follows:\n",
    "\n",
    "1. **Dimensionality Reduction**: Pooling reduces the size of feature maps, making the subsequent layers smaller and more manageable. This leads to a decrease in the number of parameters and computations required in the network, making it more efficient to train and deploy.\n",
    "\n",
    "2. **Translation Invariance**: Pooling provides the CNN with some level of spatial invariance, meaning that the network can recognize patterns even if they appear in slightly different locations within the input image. Pooling achieves this by aggregating local features and extracting their dominant characteristics, making the network less sensitive to exact positional information.\n",
    "\n",
    "3. **Feature Generalization**: By summarizing local features into more generalized representations, pooling helps prevent overfitting. Overfitting occurs when a model memorizes specific details of the training data, making it less capable of generalizing to unseen examples. Pooling enforces a form of data compression that discards fine-grained information, leading to a more robust and generalized model.\n",
    "\n",
    "4. **Noise and Distortion Robustness**: Pooling can make CNNs more robust to small changes and distortions in the input data. It captures the most prominent features and minimizes the influence of minor variations, helping the network focus on the most relevant aspects of the input.\n",
    "\n",
    "5. **Computational Efficiency**: Pooling operations significantly reduce the number of computations required for each layer, which speeds up the training and inference processes. This efficiency allows CNNs to handle larger datasets and models effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463cb523-2536-4702-a509-bef905e64726",
   "metadata": {},
   "source": [
    "### 2. Explain the difference rbtween Min pooling and Max pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005208c-91b5-4bf8-946f-b0b04447a6b4",
   "metadata": {},
   "source": [
    "It appears that there might be a typographical error in your question. I assume you are asking about the difference between \"Min Pooling\" and \"Max Pooling.\"\n",
    "\n",
    "**Max Pooling** and **Min Pooling** are both types of pooling operations used in Convolutional Neural Networks (CNNs) to downsample feature maps and reduce their spatial dimensions. However, they differ in how they aggregate information within the pooling regions.\n",
    "\n",
    "1. **Max Pooling**:\n",
    "\n",
    "   - Max pooling takes the maximum value within each pooling region and discards the other values. The output value of each pooling region is the highest activation value found in that region.\n",
    "\n",
    "   - Max pooling is commonly used because it captures the most salient features and is effective at preserving the most dominant information in the feature maps. It helps maintain robustness to translations and local variations.\n",
    "\n",
    "   - For example, in a 2x2 max pooling operation, the maximum value among four neighboring elements is selected as the output for that region.\n",
    "\n",
    "2. **Min Pooling**:\n",
    "\n",
    "   - Min pooling, on the other hand, takes the minimum value within each pooling region and discards the other values. The output value of each pooling region is the lowest activation value found in that region.\n",
    "\n",
    "   - Min pooling is less common than max pooling in CNNs and is used less frequently in practice. It has limited applications and is generally not as effective as max pooling for most computer vision tasks.\n",
    "\n",
    "   - Similar to max pooling, min pooling can also provide some form of translation invariance and noise robustness.\n",
    "\n",
    "Here's a quick comparison:\n",
    "\n",
    "- **Max Pooling**: Selects the highest activation value, preserves dominant features, widely used, provides translation invariance.\n",
    "- **Min Pooling**: Selects the lowest activation value, less commonly used, provides some translation invariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f1b02-7538-4a9b-857c-20c68fb05c62",
   "metadata": {},
   "source": [
    "### 3. Discuss the concept of padding in CNN and its significance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0768eec7-7232-4013-ae39-7599ddb012ad",
   "metadata": {},
   "source": [
    "Padding is a technique used in Convolutional Neural Networks (CNNs) to preserve the spatial dimensions of feature maps during convolution operations. It involves adding extra border pixels around the input data before applying the convolutional filters. These additional pixels are usually filled with zeros, hence the name \"zero-padding.\" The padding size is typically controlled by a parameter, and the most common choices are adding one pixel of padding on each side (usually referred to as \"same\" padding) or no padding at all (\"valid\" padding).\n",
    "\n",
    "The significance of padding in CNNs lies in several key aspects:\n",
    "\n",
    "1. **Preservation of Spatial Dimensions**: Convolutional layers without padding result in a reduction of spatial dimensions in the feature maps. If no padding is applied, the output feature map size will be smaller than the input size, and this reduction can progress with deeper layers in the network. Padding helps maintain the spatial dimensions, allowing the network to retain more spatial information and avoid information loss.\n",
    "\n",
    "2. **Centering the Convolutional Kernels**: When applying convolutional filters, the center of the kernel is usually aligned with the pixels of the input image. Without padding, the kernel's center cannot be applied to the border pixels, leading to a decrease in feature map size. Padding ensures that all pixels of the input image can be processed by the kernel, leading to consistent feature map dimensions.\n",
    "\n",
    "3. **Preserving Information at Edges**: In convolutional operations, the pixels near the edges of the input image have fewer neighboring pixels available for computation. Without padding, these edge pixels would be underrepresented in the output feature maps. Padding addresses this issue by creating a buffer zone around the input, allowing the convolution operation to capture information at the edges more effectively.\n",
    "\n",
    "4. **Mitigating the Vanishing Gradient Problem**: Padding can help mitigate the vanishing gradient problem during backpropagation, especially in deeper CNN architectures. The vanishing gradient problem occurs when gradients become too small as they are propagated back through the network during training. This can hinder the training process and reduce the model's ability to learn effectively. Padding helps retain more information in the intermediate layers, improving gradient flow and making it easier for the model to learn useful representations.\n",
    "\n",
    "5. **Facilitating Design Choices**: Padding allows greater flexibility in designing CNN architectures. It enables the use of larger convolutional kernels, which can capture more complex patterns and context. Additionally, it allows stacking multiple convolutional layers while preserving the spatial dimensions, enabling the construction of deeper networks with larger receptive fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29234b19-6bfd-4fb7-9ce0-447c1816b754",
   "metadata": {},
   "source": [
    "### 4. Compare and contrast zero-padding and valid-padding in terms of their effects on the output featuce Map size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e877af-ee9b-4c7d-85d4-80fee7296d3d",
   "metadata": {},
   "source": [
    "Zero-padding and valid-padding are two different strategies used in Convolutional Neural Networks (CNNs) that have a significant impact on the size of the output feature maps produced by convolutional layers.\n",
    "\n",
    "**1. Zero-padding:**\n",
    "- Zero-padding involves adding extra border pixels (usually filled with zeros) around the input data before applying the convolutional filters.\n",
    "- The padding size is typically controlled by a parameter, and the most common choices are adding one pixel of padding on each side (usually referred to as \"same\" padding) or more.\n",
    "- The purpose of zero-padding is to preserve the spatial dimensions of the feature maps during convolution operations.\n",
    "- When using zero-padding, the size of the output feature maps remains the same as the input size (assuming stride=1). Each pixel in the input image is processed by the convolutional kernel, and padding ensures that the kernel's center can be applied to all pixels, including those at the borders.\n",
    "- Zero-padding effectively prevents information loss, especially at the edges of the input, and allows for a better representation of the input data.\n",
    "\n",
    "**2. Valid-padding:**\n",
    "- Valid-padding, on the other hand, involves no padding at all. It means that the convolutional filters are only applied to the valid region of the input, and no extra border pixels are added.\n",
    "- Without padding, the convolutional filters can only be applied to pixels in the input image that fully overlap with the kernel.\n",
    "- As a result, the size of the output feature maps is reduced compared to the input size (unless the kernel is applied only partially to the border pixels).\n",
    "- The amount of reduction in size depends on the size of the convolutional kernel and the number of convolutional layers in the network.\n",
    "- Valid-padding is useful when you want to reduce the spatial dimensions of the feature maps, which is often desirable in deeper layers of the network to decrease the computational load and control overfitting.\n",
    "\n",
    "**Comparison:**\n",
    "- Zero-padding preserves the spatial dimensions of the feature maps, while valid-padding reduces the size of the feature maps.\n",
    "- Zero-padding keeps the output feature map size the same as the input size, while valid-padding reduces the output size based on the convolutional kernel size and the number of layers.\n",
    "- Zero-padding ensures that all pixels in the input are considered during convolution, while valid-padding only applies the convolutional filters to the valid region, neglecting the border pixels.\n",
    "- Zero-padding is useful when retaining spatial information and preventing information loss, while valid-padding is suitable for reducing spatial dimensions and controlling model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bc9f2-78d4-4ca5-a2d2-ec3219891a9a",
   "metadata": {},
   "source": [
    "## TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68be650-9481-4a12-8b7b-6d3dfa8f80bf",
   "metadata": {},
   "source": [
    "### 1. Present an overview of the AlexNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac07cdf7-a765-4132-831b-8a56445adb75",
   "metadata": {},
   "source": [
    "AlexNet is a deep Convolutional Neural Network (CNN) architecture that gained significant attention after winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton and marked a breakthrough in the field of computer vision by demonstrating the power of deep learning on large-scale image classification tasks.\n",
    "\n",
    "Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer**: The network takes an input image of size 224x224x3 (RGB color channels) as the input.\n",
    "\n",
    "2. **Convolutional Layers**: The network consists of five convolutional layers, each followed by a ReLU (Rectified Linear Unit) activation function. The filters in the early convolutional layers have small receptive fields, while the deeper layers have larger receptive fields to capture more complex patterns.\n",
    "\n",
    "3. **Max Pooling Layers**: After the first and second convolutional layers, there are max-pooling layers to reduce the spatial dimensions of the feature maps and increase the depth of the network. Max pooling is performed using 3x3 windows with a stride of 2.\n",
    "\n",
    "4. **Normalization Layers**: Local Response Normalization (LRN) layers are used to normalize the responses of neurons across different feature maps in the early layers. These layers enhance the network's ability to generalize and improve the training convergence.\n",
    "\n",
    "5. **Dropout**: Dropout layers are employed after the first three fully connected layers. Dropout randomly deactivates a certain percentage of neurons during training, reducing overfitting and improving generalization.\n",
    "\n",
    "6. **Fully Connected Layers**: The last part of the network consists of three fully connected layers. The first two fully connected layers have 4096 neurons each, and the third fully connected layer has 1000 neurons corresponding to the 1000 classes in the ImageNet dataset. These layers are followed by the Softmax activation function to obtain the final class probabilities.\n",
    "\n",
    "7. **Output Layer**: The final output is a probability distribution over the 1000 classes of the ImageNet dataset.\n",
    "\n",
    "8. **Training Details**: AlexNet was trained using the stochastic gradient descent (SGD) optimization algorithm with momentum. It also employed data augmentation techniques such as cropping, flipping, and color jittering to increase the diversity of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981a51de-772c-4774-9d7b-09c1f982cc5e",
   "metadata": {},
   "source": [
    "### 2. Explain the acrchitectural innovations introduced in AlexNet that contributed to its breakthrough perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e548f6-e9d9-4e65-9d92-0d18180c880a",
   "metadata": {},
   "source": [
    "AlexNet introduced several architectural innovations that contributed to its breakthrough performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012. These innovations were instrumental in pushing the boundaries of deep learning and image classification tasks:\n",
    "\n",
    "1. **Deep Architecture**: AlexNet was one of the first CNN architectures to have multiple layers stacked on top of each other, forming a deep network. Prior to AlexNet, most image classifiers were shallow with just a few layers. By going deeper, AlexNet was able to learn hierarchical features from raw pixels, capturing complex patterns and representations.\n",
    "\n",
    "2. **ReLU Activation Function**: Instead of using traditional activation functions like sigmoid or tanh, AlexNet used Rectified Linear Units (ReLU) as the activation function after each convolutional layer. ReLU introduces non-linearity and speeds up training by mitigating the vanishing gradient problem. The ReLU activation function allows for faster convergence and improved gradient flow during backpropagation.\n",
    "\n",
    "3. **Large Convolutional Filters**: AlexNet used large-sized filters (11x11 and 5x5) in the early convolutional layers. Larger filters can capture more spatial information and learn complex features effectively. This was in contrast to earlier networks that primarily used smaller filters.\n",
    "\n",
    "4. **Multiple GPUs for Training**: AlexNet was one of the first deep learning models to leverage multiple GPUs for parallel processing during training. This allowed faster computation and reduced the training time significantly.\n",
    "\n",
    "5. **Overlapping Max Pooling**: In the max-pooling layers, AlexNet used overlapping pooling regions. Traditional max-pooling layers used non-overlapping regions. Overlapping pooling helped avoid losing too much spatial information and resulted in better translation invariance.\n",
    "\n",
    "6. **Local Response Normalization (LRN)**: The Local Response Normalization (LRN) layers were used after the first two convolutional layers. LRN helps enhance the response of neurons by normalizing them across different feature maps within the same spatial location. It introduced local competition between neurons, which aids in generalization and helps create more robust representations.\n",
    "\n",
    "7. **Dropout Regularization**: AlexNet introduced dropout regularization after the first three fully connected layers. Dropout randomly drops out neurons during training, preventing the network from relying too much on any single feature and reducing overfitting.\n",
    "\n",
    "8. **Data Augmentation**: AlexNet used data augmentation techniques such as cropping, flipping, and color jittering during training. Data augmentation helps increase the diversity of the training data, making the model more robust and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf349dbc-2f94-49a3-b146-53609099475c",
   "metadata": {},
   "source": [
    "### 3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d8064b-57cb-413b-9bbb-748f34bb593b",
   "metadata": {},
   "source": [
    "In AlexNet, a deep Convolutional Neural Network (CNN), each type of layer (convolutional, pooling, and fully connected) plays a crucial role in learning hierarchical representations from raw pixels and ultimately classifying images.\n",
    "\n",
    "**1. Convolutional Layers:**\n",
    "- The convolutional layers are the core building blocks of CNNs. In AlexNet, there are five convolutional layers.\n",
    "- These layers consist of convolutional filters (also known as kernels) that slide over the input image, extracting local features at each position through element-wise multiplication and summation.\n",
    "- The filters in the early layers capture simple patterns like edges and textures, while filters in deeper layers learn more complex and abstract representations.\n",
    "- By stacking multiple convolutional layers, the network progressively learns hierarchical features, transforming the input image into more semantically meaningful representations.\n",
    "- Convolutional layers enable the network to learn spatially local patterns, which is crucial for image recognition tasks.\n",
    "\n",
    "**2. Pooling Layers:**\n",
    "- After some of the convolutional layers in AlexNet, there are max-pooling layers.\n",
    "- Pooling layers help reduce the spatial dimensions of the feature maps, making the network more computationally efficient and less sensitive to the exact location of features in the input image.\n",
    "- Max-pooling takes the maximum value within a region (typically 2x2) and discards the other values. This process reduces the size of the feature maps by half along each spatial dimension (assuming a stride of 2).\n",
    "- Pooling layers introduce translation invariance, meaning that the network can recognize patterns regardless of their exact positions in the input.\n",
    "- Moreover, pooling helps control overfitting by aggregating local features and extracting dominant information.\n",
    "\n",
    "**3. Fully Connected Layers:**\n",
    "- After the convolutional and pooling layers, there are three fully connected layers in AlexNet.\n",
    "- These layers connect every neuron in the previous layer to every neuron in the next layer, forming a traditional multi-layer perceptron (MLP) architecture.\n",
    "- The fully connected layers act as a classifier, taking the high-level representations learned by the preceding layers and mapping them to the output classes.\n",
    "- The last fully connected layer outputs the probabilities for each class using the Softmax activation function.\n",
    "- The number of neurons in the last fully connected layer corresponds to the number of classes in the classification task (e.g., 1000 classes in the ImageNet challenge).\n",
    "- The fully connected layers contribute to the final decision-making process, combining the spatial information learned by the convolutional layers and the abstract features learned by the pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e83db46-4fc9-4ffe-b0db-e4c14b217553",
   "metadata": {},
   "source": [
    "### 4. Implement LeNet-5 using a deep learning framework oj youy choice (e.g., TensoyFlow, PyTocch) and tyain it on a publicly available dataset (e.g., MNIST). Evaluate its perfomance and provide insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19470c-c71b-4c35-b769-398b33e322e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.2 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f2fa7cd-b28c-4d12-aab6-9bb65ad4b5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, datasets, utils\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a1f1b69-8367-485c-89c8-85f1b519429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images[..., tf.newaxis]\n",
    "test_images = test_images[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15f7b63-57ef-4fee-8b8a-34f1cbfe0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(6, (5, 5), activation='relu', input_shape=(28, 28, 1)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(16, (5, 5), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(120, activation='relu'),\n",
    "    layers.Dense(84, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5b107f9-445f-4eff-a73a-d08bce518a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a3c795-b5f6-427f-a49c-85ad8f90bd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "422/422 [==============================] - 5s 10ms/step - loss: 0.3271 - accuracy: 0.9027 - val_loss: 0.1102 - val_accuracy: 0.9672\n",
      "Epoch 2/10\n",
      "422/422 [==============================] - 4s 10ms/step - loss: 0.1030 - accuracy: 0.9690 - val_loss: 0.0924 - val_accuracy: 0.9725\n",
      "Epoch 3/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0727 - accuracy: 0.9772 - val_loss: 0.0620 - val_accuracy: 0.9812\n",
      "Epoch 4/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0589 - accuracy: 0.9817 - val_loss: 0.0593 - val_accuracy: 0.9833\n",
      "Epoch 5/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0463 - accuracy: 0.9856 - val_loss: 0.0513 - val_accuracy: 0.9857\n",
      "Epoch 6/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0385 - accuracy: 0.9876 - val_loss: 0.0532 - val_accuracy: 0.9857\n",
      "Epoch 7/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0326 - accuracy: 0.9896 - val_loss: 0.0685 - val_accuracy: 0.9807\n",
      "Epoch 8/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0300 - accuracy: 0.9901 - val_loss: 0.0483 - val_accuracy: 0.9867\n",
      "Epoch 9/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0249 - accuracy: 0.9919 - val_loss: 0.0489 - val_accuracy: 0.9877\n",
      "Epoch 10/10\n",
      "422/422 [==============================] - 4s 9ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.0494 - val_accuracy: 0.9872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f144ff19c60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, batch_size=128, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df7f5c7-f535-4b24-aedf-57040b8af04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0399 - accuracy: 0.9877\n",
      "Test accuracy: 0.9876999855041504\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff5e35-e792-4a6f-9579-fa65c3834b93",
   "metadata": {},
   "source": [
    "## TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e09f51-7449-4353-9b5c-ab43df0c380e",
   "metadata": {},
   "source": [
    "### 1. Present an overview of the AlexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dca385-9680-4101-84eb-d3f8cac1de8a",
   "metadata": {},
   "source": [
    "Certainly! Here's an overview of the AlexNet architecture:\n",
    "\n",
    "1. **Input Layer**: The AlexNet architecture takes an input image of size 227x227x3 (RGB color channels). The images are preprocessed to this fixed size before feeding them into the network.\n",
    "\n",
    "2. **Convolutional Layers**: The network consists of five convolutional layers, each followed by a Rectified Linear Unit (ReLU) activation function. The filters in the early convolutional layers have small receptive fields (e.g., 11x11, 5x5), while the deeper layers have smaller receptive fields (e.g., 3x3). This arrangement helps capture both local and more global patterns in the image.\n",
    "\n",
    "3. **Max Pooling Layers**: After some of the convolutional layers, there are max-pooling layers. Max pooling is applied with a 3x3 window and a stride of 2, reducing the spatial dimensions of the feature maps. Max pooling helps control overfitting and introduces some degree of translation invariance.\n",
    "\n",
    "4. **Normalization Layers**: Local Response Normalization (LRN) layers are used after the first two convolutional layers. LRN helps enhance the response of neurons by normalizing them across different feature maps within the same spatial location. This local competition between neurons aids in generalization and creates more robust representations.\n",
    "\n",
    "5. **Dropout**: Dropout layers are employed after the first three fully connected layers. Dropout randomly deactivates a certain percentage of neurons during training, reducing overfitting and improving generalization.\n",
    "\n",
    "6. **Fully Connected Layers**: The last part of the network consists of three fully connected layers. The first two fully connected layers have 4096 neurons each, and the third fully connected layer has 1000 neurons corresponding to the 1000 classes in the ImageNet dataset. These layers are followed by the Softmax activation function to obtain the final class probabilities.\n",
    "\n",
    "7. **Output Layer**: The final output is a probability distribution over the 1000 classes of the ImageNet dataset.\n",
    "\n",
    "8. **Training Details**: AlexNet was trained using the stochastic gradient descent (SGD) optimization algorithm with momentum. It also employed data augmentation techniques such as cropping, flipping, and color jittering to increase the diversity of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdba0e3-a392-40e6-a59a-a8a624763e91",
   "metadata": {},
   "source": [
    "### 2. Explain the architectural innovations introduced in AlexNet that contributed to its breakthrough perfomance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea50ad8-eb8a-4981-8467-34603088914b",
   "metadata": {},
   "source": [
    "AlexNet's breakthrough performance can be attributed to several key architectural innovations that pushed the boundaries of deep learning and image classification. Here are the main architectural innovations introduced in AlexNet:\n",
    "\n",
    "1. **Deep Architecture**: One of the most significant contributions of AlexNet was its deep architecture, consisting of multiple layers stacked on top of each other. Prior to AlexNet, most image classifiers were shallow with just a few layers. By going deeper, AlexNet was able to learn hierarchical features from raw pixels, capturing complex patterns and representations.\n",
    "\n",
    "2. **ReLU Activation Function**: AlexNet used Rectified Linear Units (ReLU) as the activation function after each convolutional layer, instead of traditional activation functions like sigmoid or tanh. ReLU introduces non-linearity and speeds up training by mitigating the vanishing gradient problem. The ReLU activation function allows for faster convergence and improved gradient flow during backpropagation.\n",
    "\n",
    "3. **Large Convolutional Filters**: AlexNet used large-sized filters (11x11 and 5x5) in the early convolutional layers. Larger filters can capture more spatial information and learn complex features effectively. This was in contrast to earlier networks that primarily used smaller filters.\n",
    "\n",
    "4. **Multiple GPUs for Training**: AlexNet was one of the first deep learning models to leverage multiple GPUs for parallel processing during training. This allowed faster computation and reduced the training time significantly. The ability to distribute computations across multiple GPUs was a key factor in handling the computational demands of a deep network like AlexNet.\n",
    "\n",
    "5. **Overlapping Max Pooling**: In the max-pooling layers, AlexNet used overlapping pooling regions. Traditional max-pooling layers used non-overlapping regions. Overlapping pooling helped avoid losing too much spatial information and resulted in better translation invariance.\n",
    "\n",
    "6. **Local Response Normalization (LRN)**: The Local Response Normalization (LRN) layers were used after the first two convolutional layers. LRN helps enhance the response of neurons by normalizing them across different feature maps within the same spatial location. It introduced local competition between neurons, which aids in generalization and helps create more robust representations.\n",
    "\n",
    "7. **Dropout Regularization**: AlexNet introduced dropout regularization after the first three fully connected layers. Dropout randomly drops out neurons during training, preventing the network from relying too much on any single feature and reducing overfitting.\n",
    "\n",
    "8. **Data Augmentation**: AlexNet used data augmentation techniques such as cropping, flipping, and color jittering during training. Data augmentation helps increase the diversity of the training data, making the model more robust and preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5843a3f-5557-4509-955c-2821aa0a0148",
   "metadata": {},
   "source": [
    "### 3. Discuss the role of convolutional layers, pooling layecs, and fully connected layers in AlexNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b526c-e338-4a3f-b6df-7be7128d1a61",
   "metadata": {},
   "source": [
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct roles in the process of learning hierarchical representations from raw image pixels and making predictions. Each type of layer contributes to the success of AlexNet in image classification tasks:\n",
    "\n",
    "**1. Convolutional Layers:**\n",
    "- Convolutional layers are the foundational building blocks of CNNs. In AlexNet, there are five convolutional layers.\n",
    "- The role of convolutional layers is to apply convolutional filters (also known as kernels) to the input image. These filters slide over the input image, detecting local patterns such as edges, corners, and textures.\n",
    "- Each filter learns to detect a specific feature, and the combination of multiple filters in different layers enables the network to learn hierarchical representations.\n",
    "- In AlexNet, the convolutional layers with large receptive fields (e.g., 11x11 and 5x5) capture lower-level features in the initial layers. As the network goes deeper, the convolutional layers with smaller receptive fields (e.g., 3x3) capture more complex and abstract features.\n",
    "\n",
    "**2. Pooling Layers:**\n",
    "- After some of the convolutional layers in AlexNet, there are max-pooling layers.\n",
    "- The primary role of pooling layers is to reduce the spatial dimensions of the feature maps produced by the convolutional layers. This reduces the computational complexity of the network and makes it more efficient.\n",
    "- Max pooling is used in AlexNet, which takes the maximum value within a small window (e.g., 2x2) and discards the other values. This process down-samples the feature maps and retains the most salient information.\n",
    "- Pooling layers introduce a degree of translation invariance, meaning that the network can recognize patterns regardless of their precise positions in the input.\n",
    "\n",
    "**3. Fully Connected Layers:**\n",
    "- After the convolutional and pooling layers, there are three fully connected layers in AlexNet.\n",
    "- Fully connected layers act as a classifier, taking the high-level representations learned by the preceding layers and mapping them to the output classes.\n",
    "- These layers are fully connected because each neuron in a fully connected layer is connected to all the neurons in the previous layer, forming a traditional multi-layer perceptron (MLP) architecture.\n",
    "- The fully connected layers in AlexNet enable the network to learn global patterns and correlations across the extracted features from the earlier layers.\n",
    "- The last fully connected layer in AlexNet outputs the probability distribution over the classes, and the class with the highest probability is considered the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d99595-70c6-4b46-93b5-ccd316624372",
   "metadata": {},
   "source": [
    "### 4. Implement AlexNet using a deep learning framework of your choice and evaluate its perfomance on a dataset of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30252333-fe27-43ab-9f79-d00aee5a617e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-02 01:07:09.769852: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 01:07:10.293077: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-02 01:07:10.296095: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-02 01:07:12.134677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess CIFAR-10 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "y_train = to_categorical(y_train, num_classes=10)\n",
    "y_test = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f343099-9ce7-46da-83a9-0fe8532ca289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alexnet_model():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # 1st Convolutional Layer\n",
    "    model.add(layers.Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))\n",
    "    \n",
    "    # 2nd Convolutional Layer\n",
    "    model.add(layers.Conv2D(256, (5, 5), padding='same', activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    # 3rd Convolutional Layer\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # 4th Convolutional Layer\n",
    "    model.add(layers.Conv2D(384, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # 5th Convolutional Layer\n",
    "    model.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "    # Fully Connected Layers\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(4096, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6912053c-3623-4f19-b6e6-9f09c6305ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Create and compile the model\n",
    "model = alexnet_model()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64abc63e-a79c-42e9-a4d6-29d969e70be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "352/352 [==============================] - 55s 152ms/step - loss: 2.0540 - accuracy: 0.1912 - val_loss: 2.0022 - val_accuracy: 0.2098\n",
      "Epoch 2/4\n",
      "352/352 [==============================] - 48s 136ms/step - loss: 1.8640 - accuracy: 0.2658 - val_loss: 1.7626 - val_accuracy: 0.3136\n",
      "Epoch 3/4\n",
      "352/352 [==============================] - 48s 135ms/step - loss: 1.7373 - accuracy: 0.3313 - val_loss: 1.7178 - val_accuracy: 0.3402\n",
      "Epoch 4/4\n",
      "352/352 [==============================] - 47s 134ms/step - loss: 1.6770 - accuracy: 0.3694 - val_loss: 1.6105 - val_accuracy: 0.3932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2ac129cd90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=4, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e0edec9-4c20-4d34-8315-4c664fabab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 14ms/step - loss: 1.6164 - accuracy: 0.4001\n",
      "Test accuracy: 0.4000999927520752\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=1)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
