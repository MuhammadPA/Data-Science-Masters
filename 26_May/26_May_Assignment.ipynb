{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30dc36f1-afd7-4ba4-a825-2474e18ef2ff",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343ebcb1-1325-4a5f-91eb-2147c4827d22",
   "metadata": {},
   "source": [
    "### 1. What is regularization in the context of deep learning. Why is it important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8ba2d4-2951-4853-ae04-321d975dff48",
   "metadata": {},
   "source": [
    "In the context of deep learning, regularization refers to a set of techniques used to prevent overfitting and improve the generalization capability of a neural network model. Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data.\n",
    "\n",
    "Regularization methods introduce additional constraints or penalties on the model's parameters during the training process. These constraints help to control the complexity of the model and discourage it from fitting the noise or irrelevant patterns in the training data.\n",
    "\n",
    "There are different types of regularization techniques commonly used in deep learning:\n",
    "\n",
    "1. L1 and L2 Regularization (Weight Decay): L1 and L2 regularization, also known as weight decay, add a penalty term to the loss function that is proportional to either the absolute values of the model weights (L1) or the squared values of the model weights (L2). This encourages the model to learn smaller weights, reducing the overall complexity of the model.\n",
    "\n",
    "2. Dropout: Dropout is a technique that randomly sets a fraction of the neurons in a layer to zero during each training iteration. This helps in preventing complex co-adaptations between neurons, making the network more robust and reducing overfitting.\n",
    "\n",
    "3. Early Stopping: Early stopping involves monitoring the performance of the model on a validation set during training and stopping the training process when the performance starts to deteriorate. This prevents the model from continuing to learn the noise in the training data and helps in finding the point of best generalization.\n",
    "\n",
    "Regularization is important in deep learning for several reasons:\n",
    "\n",
    "1. Preventing Overfitting: Regularization techniques help to reduce overfitting, which occurs when the model becomes too complex and starts memorizing the training examples instead of learning the underlying patterns. Regularization encourages the model to focus on the most important features and prevents it from being overly sensitive to noise or outliers in the training data.\n",
    "\n",
    "2. Improving Generalization: By reducing overfitting, regularization techniques improve the model's ability to generalize to unseen data. A regularized model tends to perform better on new, unseen examples by capturing the underlying patterns and avoiding over-reliance on specific training instances.\n",
    "\n",
    "3. Handling Limited Data: In situations where the available training data is limited, regularization can play a crucial role. By controlling the complexity of the model, regularization helps to prevent overfitting even with a small training set, improving the model's ability to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e631ac5a-88b4-4c26-b6cf-23aaf8cd2f91",
   "metadata": {},
   "source": [
    "### 2. Explain the bias-variance tradeoff and how regularization helps in addressing this tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2898475-43ac-4377-a491-a2269c509d0c",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the relationship between a model's bias and variance and their impact on its predictive performance.\n",
    "\n",
    "- Bias refers to the error introduced by approximating a real-world problem with a simplified model. A model with high bias tends to make strong assumptions about the data, leading to underfitting. Underfitting occurs when the model is too simplistic to capture the underlying patterns in the data, resulting in poor performance on both the training and test data.\n",
    "\n",
    "- Variance, on the other hand, refers to the model's sensitivity to fluctuations in the training data. A model with high variance is excessively complex and captures noise or random fluctuations in the training data, leading to overfitting. Overfitting occurs when the model fits the training data too closely, but fails to generalize well to new, unseen data.\n",
    "\n",
    "The bias-variance tradeoff arises because reducing bias often increases variance, and reducing variance often increases bias. Finding the right balance between bias and variance is crucial for building models that generalize well.\n",
    "\n",
    "Regularization helps in addressing the bias-variance tradeoff by controlling the complexity of the model and reducing overfitting:\n",
    "\n",
    "1. Bias Reduction: Regularization techniques such as L1 and L2 regularization introduce a penalty on the model's parameters during training. This penalty encourages the model to learn smaller weights, reducing its complexity and bias. By preventing the model from becoming too simplistic, regularization helps to mitigate underfitting.\n",
    "\n",
    "2. Variance Reduction: Regularization also helps to reduce variance by discouraging the model from fitting noise or irrelevant patterns in the training data. By penalizing large weights, regularization encourages the model to focus on the most important features and reduces its sensitivity to fluctuations in the training data. Techniques like dropout, which randomly deactivate neurons during training, also help in reducing variance by preventing complex co-adaptations between neurons.\n",
    "\n",
    "By reducing both bias and variance, regularization techniques aim to find an optimal balance that minimizes the overall error of the model. This leads to improved generalization performance, where the model can accurately predict outcomes for unseen data by capturing the underlying patterns without overfitting to noise or irrelevant details in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507ce72-33c5-462e-9061-03b25c0a8eb6",
   "metadata": {},
   "source": [
    "### 3. Describe the concept of L1 and L2 regularization. How do they differ in terms of penalty calculation and their effects on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da92488-b85f-4005-a4f9-671c7834ae4d",
   "metadata": {},
   "source": [
    "L1 and L2 regularization are commonly used techniques to apply regularization in machine learning models, including deep learning models. They differ in terms of penalty calculation and their effects on the model's parameters.\n",
    "\n",
    "1. L1 Regularization (Lasso Regularization):\n",
    "L1 regularization adds a penalty term to the loss function that is proportional to the sum of the absolute values of the model's weights. The penalty term is calculated as the L1 norm (also known as the Manhattan norm) of the weight vector.\n",
    "\n",
    "Penalty calculation: L1 penalty = λ * ||w||₁\n",
    "\n",
    "Effect on the model:\n",
    "- L1 regularization encourages sparsity in the model, meaning it encourages some of the model's weights to become exactly zero. This leads to a sparse model where only a subset of the features has non-zero weights, effectively performing feature selection.\n",
    "- By forcing some weights to zero, L1 regularization can help in reducing the complexity of the model, removing irrelevant or redundant features, and improving interpretability.\n",
    "- The sparsity induced by L1 regularization makes the model more robust to noisy or irrelevant features and reduces the risk of overfitting.\n",
    "\n",
    "2. L2 Regularization (Ridge Regularization):\n",
    "L2 regularization adds a penalty term to the loss function that is proportional to the sum of the squared values of the model's weights. The penalty term is calculated as the L2 norm (also known as the Euclidean norm) of the weight vector.\n",
    "\n",
    "Penalty calculation: L2 penalty = λ * ||w||₂²\n",
    "\n",
    "Effect on the model:\n",
    "- L2 regularization encourages the weights to be small but does not force them to be exactly zero. It reduces the magnitude of all weights equally, without eliminating any features entirely.\n",
    "- By reducing the magnitude of weights, L2 regularization helps in controlling the overall complexity of the model and prevents large weights that can cause overfitting.\n",
    "- L2 regularization can improve the model's generalization performance by preventing the model from relying too heavily on specific training instances or noise in the data.\n",
    "- L2 regularization is computationally efficient and generally leads to faster convergence during training compared to L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a9ee6e-fb15-4b62-9336-549851bfa449",
   "metadata": {},
   "source": [
    "### 4. Discuss the role of regularization in preventing overfitting and improving the generalization of deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d68c92-690e-4ea9-b7fd-dec2ffa1c8b9",
   "metadata": {},
   "source": [
    "Regularization plays a crucial role in preventing overfitting and improving the generalization of deep learning models. Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. Regularization techniques help address overfitting by introducing additional constraints or penalties on the model's parameters during the training process. Here's how regularization achieves this:\n",
    "\n",
    "1. Controlling Model Complexity: Regularization techniques, such as L1 and L2 regularization, add a penalty term to the loss function that discourages the model from learning overly complex representations. By penalizing large weights or encouraging sparsity, regularization limits the model's capacity to fit the noise or irrelevant patterns in the training data. This constraint helps prevent the model from becoming overly complex and overfitting the training data.\n",
    "\n",
    "2. Feature Selection: Regularization techniques, particularly L1 regularization, promote sparsity by encouraging some of the model's weights to become exactly zero. This feature selection property allows the model to focus on the most informative features while ignoring irrelevant or redundant ones. By eliminating irrelevant features, regularization reduces the risk of overfitting and helps the model generalize better to unseen data.\n",
    "\n",
    "3. Reducing Sensitivity to Training Data: Regularization techniques, such as dropout, randomly deactivate neurons during training, preventing complex co-adaptations between neurons. This process introduces noise and perturbations into the model, making it more robust and less sensitive to specific training examples. By reducing sensitivity to individual training instances, regularization helps the model learn more generalizable representations that can better handle variations and noise in unseen data.\n",
    "\n",
    "4. Handling Limited Data: In situations where the available training data is limited, regularization becomes even more critical. With a small training set, there is a higher risk of overfitting and memorizing noise. Regularization techniques effectively regularize the model's learning process, making it less prone to overfitting even with limited data. By preventing the model from fitting the noise in the training set, regularization helps improve generalization performance.\n",
    "\n",
    "5. Early Stopping: Although not strictly a regularization technique, early stopping is often employed in combination with regularization. Early stopping involves monitoring the model's performance on a validation set during training and stopping the training process when the performance starts deteriorating. Regularization aids in early stopping by preventing the model from overfitting and continuing to improve performance on the training data while failing to generalize to new data. This allows the model to be trained up to the point of optimal generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4ee81d-6d8e-46bd-a78c-9c30f6c42659",
   "metadata": {},
   "source": [
    "## Part 2: Regularization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c770f-bc14-4cd9-8146-29368483f5b9",
   "metadata": {},
   "source": [
    "### 5. Explain Dropout regularization and how it works to reduce overfitting. Discuss the impact of Dropout on model training and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7bfa7b-e6a1-4a17-91a9-ece4893f56b0",
   "metadata": {},
   "source": [
    "Dropout regularization is a widely used technique in deep learning that helps reduce overfitting by preventing complex co-adaptations among neurons. It works by randomly deactivating a fraction of the neurons during each training iteration.\n",
    "\n",
    "Here's how dropout regularization works and its impact on model training and inference:\n",
    "\n",
    "1. Dropout during Training:\n",
    "During training, dropout is applied by randomly selecting a subset of neurons in a layer and setting their outputs to zero. The selection is performed independently for each training example and each training iteration. This means that each neuron has a probability (usually denoted as p) of being dropped, and the probability can vary across iterations.\n",
    "\n",
    "By randomly dropping neurons, dropout prevents complex co-adaptations among them because the remaining neurons must compensate for the deactivated ones. This encourages the network to learn more robust and generalizable features instead of relying on specific subsets of neurons. Dropout effectively acts as a form of ensemble learning, where different subsets of neurons are trained on different subsets of the data, leading to improved generalization.\n",
    "\n",
    "2. Impact on Model Training:\n",
    "The impact of dropout regularization on model training includes:\n",
    "\n",
    "- Increased Robustness: Dropout introduces noise and perturbations into the model during training, making it more robust. The model becomes less sensitive to specific training examples and can generalize better to unseen data.\n",
    "\n",
    "- Reduced Overfitting: By preventing complex co-adaptations, dropout regularization reduces the risk of overfitting. It helps the model avoid memorizing noise or idiosyncrasies in the training data and encourages it to focus on the most informative features.\n",
    "\n",
    "- Smoother Convergence: Dropout can result in a slower convergence rate during training because the model is constantly adapting to the random deactivation of neurons. However, this slower convergence often leads to better generalization performance.\n",
    "\n",
    "3. Impact on Model Inference:\n",
    "During inference (when the trained model is used to make predictions on new, unseen data), dropout is typically turned off. The full model, including all the neurons, is used for making predictions. However, the learned weights are scaled to account for the effect of dropout during training.\n",
    "\n",
    "The scaling of weights during inference is done to ensure that the expected output of each neuron is the same during training and inference. It is achieved by multiplying the weights by the probability (1 - p) at each layer. This scaling accounts for the fact that during training, only a fraction (p) of the neurons were active, but during inference, all neurons are active.\n",
    "\n",
    "The use of dropout during training and its scaling during inference allows the model to benefit from the regularization effect of dropout during training while ensuring consistent behavior during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc09f26-b80e-49d3-882a-b6dd59d27097",
   "metadata": {},
   "source": [
    "### 6. Describe the concept of Early stopping as a form of regularization. How does it help prevent overfitting during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3958ca53-f155-40d0-a469-096b3b843952",
   "metadata": {},
   "source": [
    "Early stopping is a form of regularization that helps prevent overfitting during the training process of machine learning models, including deep learning models. It involves monitoring the model's performance on a validation set during training and stopping the training process when the performance on the validation set starts to deteriorate.\n",
    "\n",
    "Here's how early stopping works and how it helps prevent overfitting:\n",
    "\n",
    "1. Training and Validation Sets:\n",
    "During model training, the available data is typically split into three sets: a training set, a validation set, and a test set. The training set is used to update the model's parameters, the validation set is used to monitor the model's performance during training, and the test set is used to evaluate the final performance of the trained model.\n",
    "\n",
    "2. Monitoring Validation Performance:\n",
    "At the end of each training iteration (epoch), the model's performance is evaluated on the validation set. The performance metric used can vary based on the problem, such as accuracy, loss, or any other relevant metric. The validation performance is tracked throughout the training process.\n",
    "\n",
    "3. Early Stopping Criterion:\n",
    "Early stopping involves defining a criterion or rule to determine when to stop the training process. The criterion is typically based on the validation performance. A common approach is to track the validation loss or error and stop training when the validation loss starts to increase consistently or when the validation error starts to worsen.\n",
    "\n",
    "4. Preventing Overfitting:\n",
    "By monitoring the validation performance and stopping the training process when the model's performance on the validation set starts to deteriorate, early stopping prevents overfitting. Overfitting occurs when the model becomes too complex and starts to memorize the training data, resulting in poor performance on new, unseen data.\n",
    "\n",
    "Early stopping helps prevent overfitting in the following ways:\n",
    "\n",
    "- Timely Stopping: Early stopping stops the training process before the model has a chance to overfit the training data excessively. It identifies the point where the model achieves the best tradeoff between training performance and generalization performance.\n",
    "\n",
    "- Generalization Improvement: As the training progresses, the model's performance on the training set may continue to improve, but it might not generalize well to new data. Early stopping prevents the model from fitting noise or irrelevant patterns in the training data, as it prioritizes generalization performance over further improvement on the training set.\n",
    "\n",
    "- Simplicity and Complexity Control: Early stopping helps to find a simpler model that can generalize better. By stopping the training process at an earlier stage, the model is effectively constrained in terms of complexity, reducing the risk of overfitting and improving generalization.\n",
    "\n",
    "It's important to note that early stopping should be used in conjunction with other regularization techniques, such as L1 or L2 regularization, to further enhance the model's ability to prevent overfitting and improve generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd0e51-90a5-4a4d-8984-61f114eda786",
   "metadata": {},
   "source": [
    "### 7. Explain the concept of Batch Normalization and its role as a form of regularization. How does Batch Normalization help in preventing overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba9df52-dbe0-4c1a-be09-ea7a62f05a4d",
   "metadata": {},
   "source": [
    "Batch Normalization is a technique used in deep learning to normalize the inputs of each layer by adjusting and scaling the activations. It aims to address the internal covariate shift problem and has the additional benefit of acting as a form of regularization.\n",
    "\n",
    "Here's how Batch Normalization works and how it helps prevent overfitting:\n",
    "\n",
    "1. Normalization within a Mini-Batch:\n",
    "During training, Batch Normalization operates on mini-batches of data. For each mini-batch, the mean and standard deviation of the activations across the mini-batch are computed. Then, the activations are normalized by subtracting the mean and dividing by the standard deviation.\n",
    "\n",
    "2. Scaling and Shifting:\n",
    "After normalization, the normalized activations are multiplied by a learnable scaling parameter (gamma) and added to a learnable shifting parameter (beta). These parameters allow the network to learn the optimal scaling and shifting for the normalized activations.\n",
    "\n",
    "3. Role as Regularization:\n",
    "Batch Normalization acts as a form of regularization by introducing noise during training. The normalization step adds some randomness to the activations within each mini-batch, which can be seen as a form of noise injection.\n",
    "\n",
    "The regularization effect of Batch Normalization helps prevent overfitting in the following ways:\n",
    "\n",
    "- Reducing Internal Covariate Shift: Internal covariate shift refers to the change in the distribution of layer inputs during training. By normalizing the activations, Batch Normalization reduces the internal covariate shift. This stabilization allows for more stable and efficient learning, making the training process less susceptible to overfitting.\n",
    "\n",
    "- Smoother Optimization: Batch Normalization normalizes the gradients that flow backward through the network. This smoothing effect makes the optimization process more stable, allowing for faster convergence and reducing the risk of overfitting.\n",
    "\n",
    "- Reducing Reliance on Specific Weights: Batch Normalization reduces the dependence of the network on specific weights. By normalizing the activations, it makes the network less sensitive to the scale and initialization of the weights. This reduces the risk of overfitting to specific weight configurations and allows for better generalization.\n",
    "\n",
    "- Allowing Higher Learning Rates: The normalization of activations in Batch Normalization helps in preventing gradient explosion or vanishing, allowing for the use of higher learning rates during training. Higher learning rates can accelerate convergence and improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b95ec4-8199-4ad7-9371-38864d6ff11e",
   "metadata": {},
   "source": [
    "## Part 3: Applying Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c53268-77e2-4622-afe2-fbd2c4554639",
   "metadata": {},
   "source": [
    "### 8. Implement Dropout regularization in a deep learning model using a framework of your choice. Evaluate its impact on model performance and compare it with a model without Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "328f9937-fb51-4068-bc76-aedc0fa107e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58\n",
      "  Downloading nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.4.91\n",
      "  Downloading nvidia_cusparse_cu11-11.7.4.91-py3-none-manylinux1_x86_64.whl (173.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.0.0\n",
      "  Downloading triton-2.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (63.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting nvidia-nccl-cu11==2.14.3\n",
      "  Downloading nvidia_nccl_cu11-2.14.3-py3-none-manylinux1_x86_64.whl (177.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu11==11.7.91\n",
      "  Downloading nvidia_nvtx_cu11-11.7.91-py3-none-manylinux1_x86_64.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cusolver-cu11==11.4.0.1\n",
      "  Downloading nvidia_cusolver_cu11-11.4.0.1-2-py3-none-manylinux1_x86_64.whl (102.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu11==10.2.10.91\n",
      "  Downloading nvidia_curand_cu11-10.2.10.91-py3-none-manylinux1_x86_64.whl (54.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.7.101\n",
      "  Downloading nvidia_cuda_cupti_cu11-11.7.101-py3-none-manylinux1_x86_64.whl (11.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (65.5.1)\n",
      "Collecting lit\n",
      "  Downloading lit-16.0.6.tar.gz (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.7/153.7 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cmake\n",
      "  Downloading cmake-3.26.4-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Building wheels for collected packages: lit\n",
      "  Building wheel for lit (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lit: filename=lit-16.0.6-py3-none-any.whl size=93582 sha256=e2f1e9d70487161928fcb5ef809db3fd5646cb51a6311a4705cefc2e22f4d802\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/1a/56/14/294a6c208bce35b0fc3170fe1049b2fd3f61ce6495fc3870b3\n",
      "Successfully built lit\n",
      "Installing collected packages: lit, cmake, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, filelock, nvidia-cusolver-cu11, nvidia-cudnn-cu11, triton, torch\n",
      "Successfully installed cmake-3.26.4 filelock-3.12.2 lit-16.0.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-cupti-cu11-11.7.101 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.2.10.91 nvidia-cusolver-cu11-11.4.0.1 nvidia-cusparse-cu11-11.7.4.91 nvidia-nccl-cu11-2.14.3 nvidia-nvtx-cu11-11.7.91 torch-2.0.1 triton-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b77fe44-65de-4216-8f6c-2c36290d91d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.15.2-cp310-cp310-manylinux1_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch==2.0.1 in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.0.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.8.8)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (4.4.0)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (11.7.4.91)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (1.11.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.1.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.0.1->torchvision) (3.12.2)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchvision) (65.5.1)\n",
      "Requirement already satisfied: cmake in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (3.26.4)\n",
      "Requirement already satisfied: lit in /opt/conda/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->torchvision) (16.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.0.1->torchvision) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.0.1->torchvision) (1.2.1)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.15.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4a9d95d-4759-4b09-ba65-cf8d70d0d94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelWithoutDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithoutDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3f9d86f-1fbd-467d-ac06-911ee223663f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ModelWithDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout probability of 0.5\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2aae4079-6967-49dd-8f34-d373b313d10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 175433824.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 64126889.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 271776295.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 7681664.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Accuracy without Dropout: 0.9807\n",
      "Accuracy with Dropout: 0.981\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Load the MNIST dataset\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=ToTensor())\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the training function\n",
    "def train(model, optimizer, criterion, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# Define the evaluation function\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Create the models\n",
    "model_without_dropout = ModelWithoutDropout()\n",
    "model_with_dropout = ModelWithDropout()\n",
    "\n",
    "# Define the optimizer and loss criterion\n",
    "optimizer_without_dropout = optim.Adam(model_without_dropout.parameters(), lr=0.001)\n",
    "optimizer_with_dropout = optim.Adam(model_with_dropout.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the models\n",
    "train(model_without_dropout, optimizer_without_dropout, criterion, epochs=10)\n",
    "train(model_with_dropout, optimizer_with_dropout, criterion, epochs=10)\n",
    "\n",
    "# Evaluate the models\n",
    "accuracy_without_dropout = evaluate(model_without_dropout)\n",
    "accuracy_with_dropout = evaluate(model_with_dropout)\n",
    "\n",
    "print(\"Accuracy without Dropout:\", accuracy_without_dropout)\n",
    "print(\"Accuracy with Dropout:\", accuracy_with_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93026bb-29c6-4d6a-915e-7e43aea3cbab",
   "metadata": {},
   "source": [
    "### 9.Discuss the considerations and tradeoffs when choosing the appropriate regularization technique for a given deep learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f192cfe-4bae-48af-8301-f93a6249bfea",
   "metadata": {},
   "source": [
    "When choosing the appropriate regularization technique for a deep learning task, there are several considerations and tradeoffs to take into account. Here are some key factors to consider:\n",
    "\n",
    "1. Problem Complexity: The complexity of the problem at hand plays a crucial role in selecting the appropriate regularization technique. If the problem is relatively simple and the model is not at a high risk of overfitting, simpler regularization techniques like L2 regularization or early stopping might suffice. On the other hand, for more complex problems or models, more advanced techniques like Dropout or Batch Normalization may be necessary.\n",
    "\n",
    "2. Model Architecture: The choice of regularization technique can also depend on the specific architecture of the deep learning model. Different regularization methods may have varying effects on different types of architectures. For example, techniques like Dropout and Batch Normalization are commonly used in fully connected or convolutional neural networks, while recurrent neural networks may benefit more from techniques like recurrent dropout or recurrent batch normalization.\n",
    "\n",
    "3. Available Data: The size and quality of the available data influence the choice of regularization technique. If the dataset is large and diverse, the risk of overfitting may be reduced, and simpler regularization techniques may be sufficient. However, in cases where the dataset is small or contains noisy or imbalanced samples, more advanced techniques like Dropout or data augmentation may be necessary to prevent overfitting.\n",
    "\n",
    "4. Interpretability vs. Performance: Consider the balance between interpretability and performance. Some regularization techniques, like L1 regularization, can induce sparsity and feature selection, making the model more interpretable. However, these techniques may come at the cost of slightly reduced performance compared to other regularization methods that do not explicitly promote sparsity.\n",
    "\n",
    "5. Computational Complexity: Different regularization techniques have varying computational costs. Some methods, like L1 and L2 regularization, are computationally efficient and have a minimal impact on training time. However, more complex techniques like Dropout or Batch Normalization may increase training time due to the additional computations involved in the forward and backward passes.\n",
    "\n",
    "6. Hyperparameter Tuning: Most regularization techniques involve hyperparameters that need to be tuned. Consider the effort and resources required for hyperparameter tuning. Techniques with fewer hyperparameters, such as L1 and L2 regularization, may be easier to tune, while others like Dropout or Batch Normalization may require more careful selection of hyperparameters.\n",
    "\n",
    "7. Previous Empirical Success: Consider previous empirical success or domain-specific knowledge with regularization techniques for similar tasks. Explore existing literature and research to understand which regularization techniques have shown promise in similar scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
