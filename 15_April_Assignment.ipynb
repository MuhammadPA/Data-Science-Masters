{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46bb97ad-266c-414e-a11c-8bb624521790",
   "metadata": {},
   "source": [
    "### 1. You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values.\n",
    "\n",
    "Design a pipeline that includes the following steps\"\n",
    "\n",
    "Use an automated feature selection method to identify the important features in the dataset\n",
    "\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the numer#cal columns using the mean of the column values\n",
    "\n",
    "Scale the numerical columns using standardisation\n",
    "\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the categorical columns using the most frequent value of the column\n",
    "\n",
    "One-hot encode the categorical columns\n",
    "\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformer\n",
    "\n",
    "Use a Random Forest Classifier to build the final model\n",
    "\n",
    "Evaluate the accuracy of the model on the test dataset\n",
    "\n",
    "Note! Your solution should include code snipets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also proide an interpretation of the results and suggest possible improvements for\n",
    "the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660ed9c-3e78-46e8-bcb9-0993d4444808",
   "metadata": {},
   "source": [
    "Certainly! Here's an example of a pipeline that includes the steps you mentioned:\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming we have X as the feature matrix and y as the target variable\n",
    "\n",
    "# Step 1: Automated feature selection\n",
    "feature_selector = SelectFromModel(RandomForestClassifier())\n",
    "\n",
    "# Step 2: Numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Step 4: Combine numerical and categorical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "# Step 5: Final pipeline with preprocessor and classifier\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selector', feature_selector),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Step 6: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 7: Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions on the test data\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "```\n",
    "\n",
    "Explaination of the steps \n",
    "\n",
    "1. Automated Feature Selection: SelectFromModel uses a model (in this case, Random Forest Classifier) to automatically select important features.\n",
    "\n",
    "2. Numerical Pipeline: Handles missing values in numerical columns by imputing them with the mean value of the column. Scales the numerical columns using standardization.\n",
    "\n",
    "3. Categorical Pipeline: Handles missing values in categorical columns by imputing them with the most frequent value of the column. One-hot encodes the categorical columns.\n",
    "\n",
    "4. ColumnTransformer: Combines the numerical and categorical pipelines, applying the appropriate preprocessing steps to each type of column.\n",
    "\n",
    "5. Final Pipeline: Includes the feature selection, preprocessor, and Random Forest Classifier.\n",
    "\n",
    "6. Split the Data: Splits the data into training and test sets for model evaluation.\n",
    "\n",
    "7. Fit the Pipeline: Fits the pipeline to the training data, performing all the preprocessing steps and training the model.\n",
    "\n",
    "8. Make Predictions: Uses the trained pipeline to make predictions on the test data.\n",
    "\n",
    "9. Evaluate Accuracy: Compares the predicted labels with the true labels and calculates the accuracy of the model.\n",
    "\n",
    "Interpretation and Possible Improvements:\n",
    "The pipeline automates the feature engineering process and handles missing values in both numerical and categorical features. The Random Forest Classifier is used as the final model for classification.\n",
    "\n",
    "Interpreting the accuracy score obtained from the pipeline allows you to assess the performance of the model on the test dataset. Higher accuracy indicates better performance. However, it's essential to consider other evaluation metrics and conduct further analysis, such as examining the confusion matrix or calculating precision, recall, or F1-score.\n",
    "\n",
    "Possible improvements to the pipeline depend on the specific characteristics of your dataset and problem:\n",
    "\n",
    "1. Hyperparameter Tuning: Explore different hyperparameter values for the Random Forest Classifier to optimize its performance. Consider using techniques like grid search or randomized search.\n",
    "\n",
    "\n",
    "\n",
    "2. Feature Engineering: Experiment with additional feature engineering techniques, such as creating interaction terms, polynomial features, or domain-specific transformations, to capture more complex relationships in the data.\n",
    "\n",
    "3. Handling Imbalanced Classes: If the dataset is imbalanced, where one class dominates the other, consider using techniques like oversampling, undersampling, or class weighting to address the class imbalance problem.\n",
    "\n",
    "4. Model Selection: Besides Random Forest, consider trying other classification algorithms to see if they yield better results for your specific dataset, such as Gradient Boosting, Support Vector Machines, or Neural Networks.\n",
    "\n",
    "It's crucial to adapt and fine-tune the pipeline based on the characteristics of your dataset and the specific requirements of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac195dde-4bc5-4934-8246-072ba1b735ef",
   "metadata": {},
   "source": [
    "### 2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480dffc4-b3f6-4f7f-82e1-679bb88c1d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Step 1: Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Step 2: Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Define the individual classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Step 4: Define the Voting Classifier combining the individual classifiers\n",
    "voting_classifier = VotingClassifier(\n",
    "    estimators=[('rf', rf_classifier), ('lr', lr_classifier)],\n",
    "    voting='hard'  # Use majority voting\n",
    ")\n",
    "\n",
    "# Step 5: Fit the Voting Classifier to the training data\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Make predictions on the test data\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "\n",
    "# Step 7: Evaluate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
