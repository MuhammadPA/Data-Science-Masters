{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20843df5-6bd7-4ed9-985d-f7ca4c551d52",
   "metadata": {},
   "source": [
    "### 1. What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b752171-48a9-4f02-bcd1-203b52a9c0d8",
   "metadata": {},
   "source": [
    "Missing values in a dataset are values that are not recorded or are absent for certain observations in one or more variables. These missing values can occur for various reasons such as human errors, data entry problems, measurement errors, or incomplete surveys.\n",
    "\n",
    "It is essential to handle missing values because they can adversely affect the statistical analyses and machine learning models that are built on the dataset. Missing values can lead to biased estimates, reduced statistical power, and erroneous conclusions. Moreover, many machine learning algorithms cannot handle missing values and may produce errors or incorrect predictions.\n",
    "\n",
    "Some of the algorithms that are not affected by missing values are:\n",
    "\n",
    "1.Decision Trees: Decision trees can handle missing values by treating them as a separate category and using surrogate splits to fill in the missing values.\n",
    "\n",
    "2.Random Forest: Random forest can handle missing values by imputing the missing values using the mode of the non-missing values in the same feature.\n",
    "\n",
    "3.K-Nearest Neighbors: KNN can handle missing values by ignoring the missing values and computing distances using the available values in the feature vector.\n",
    "\n",
    "4.Support Vector Machines: SVM can handle missing values by omitting the missing values from the kernel function and using only the available values in the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f275e56-8483-4cc0-9829-28e5ecbd4180",
   "metadata": {},
   "source": [
    "### 2. List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5421f6f6-cb3c-4c7c-b123-1f0ac6cabf3f",
   "metadata": {},
   "source": [
    "There are several techniques used to handle missing data in a dataset. Here are some of the commonly used techniques along with an example of each using Python:\n",
    "\n",
    "Deletion: In this technique, we remove the rows or columns that contain missing data. This technique is simple but can lead to a loss of valuable information.\n",
    "\n",
    "Imputation: In this technique, we replace missing data with estimated or predicted values. There are several methods for imputation such as mean imputation, median imputation, and mode imputation.\n",
    "\n",
    "Forward Fill: In this technique, we replace missing data with the last observed value. This method is useful when the data has a temporal or sequential nature.\n",
    "\n",
    "Backward Fill: In this technique, we replace missing data with the next observed value. This method is also useful when the data has a temporal or sequential nature.\n",
    "\n",
    "K-Nearest Neighbor (KNN) Imputation: In this technique, we replace missing data with values of its k-nearest neighbors. This method is useful when the missing values have a strong relationship with other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59d680d8-b491-43d2-9bed-117292aeaf14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C\n",
       "0  1.0  1.0  NaN\n",
       "1  2.0  NaN  2.0\n",
       "2  NaN  3.0  3.0\n",
       "3  4.0  4.0  NaN\n",
       "4  5.0  5.0  5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deletion\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4, 5], 'B': [1, None, 3, 4, 5], 'C': [None, 2, 3, None, 5]}\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d13ac3-8dc5-4a65-919d-d66f4332eddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C\n",
       "4  5.0  5.0  5.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with missing values\n",
    "df_drop_rows = df.dropna()\n",
    "df_drop_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a1efb7b-66f5-4bd4-bb79-3295e72ac84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns with missing values\n",
    "df_drop_cols = df.dropna(axis=1)\n",
    "df_drop_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ca08d-bdd1-4da7-8451-eb162259ac84",
   "metadata": {},
   "source": [
    "### 3. Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866fb497-b09c-4be2-b2b9-4d0ad4a0a27a",
   "metadata": {},
   "source": [
    "Imbalanced data refers to a situation where the distribution of classes in a dataset is disproportionate. This means that one class has significantly more or fewer examples than another class.\n",
    "\n",
    "For example, suppose you have a binary classification problem where you want to classify whether a customer is going to churn or not. In this case, if 95% of the customers are not churning, and only 5% are churning, then the data is said to be imbalanced.\n",
    "\n",
    "If imbalanced data is not handled properly, it can lead to several issues, such as:\n",
    "\n",
    "Bias towards the majority class: Since the majority class has more examples, the model might learn to predict this class always. This can result in poor performance for the minority class, leading to poor predictions and a lower overall accuracy.\n",
    "\n",
    "Poor generalization: In some cases, the model may overfit the majority class and perform poorly on the test data, leading to poor generalization.\n",
    "\n",
    "Misleading evaluation metrics: Accuracy is not an appropriate metric to evaluate the performance of models trained on imbalanced datasets. Even a model that always predicts the majority class would have a high accuracy. In such cases, alternative metrics such as precision, recall, F1-score, or AUC-ROC should be used.\n",
    "\n",
    "Loss of information: In cases where the minority class is essential, a model that only focuses on the majority class can lead to a loss of valuable information and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149869de-33e6-4395-9b05-a46c821cb8e0",
   "metadata": {},
   "source": [
    "### 4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bddb89-d9c6-4d65-bc21-b80b782fe897",
   "metadata": {},
   "source": [
    "Upsampling and downsampling are two techniques used to balance imbalanced data by adjusting the distribution of classes.\n",
    "\n",
    "Upsampling is a technique used to increase the number of samples in the minority class by creating new samples. This can be done by either replicating existing samples or generating new synthetic samples using techniques such as SMOTE (Synthetic Minority Over-sampling Technique). For example, suppose you have a dataset with two classes, A and B, where class A has only 100 samples, and class B has 1000 samples. In this case, you can use upsampling to generate new samples for class A to increase its size and balance the distribution of classes.\n",
    "\n",
    "Downsampling is a technique used to reduce the number of samples in the majority class by removing some samples. This can be done randomly or using other techniques such as Tomek links, which remove samples that are close to samples of the other class. For example, suppose you have a dataset with two classes, A and B, where class A has 1000 samples, and class B has only 100 samples. In this case, you can use downsampling to reduce the size of class A and balance the distribution of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde491fe-79c0-4edd-bc9f-bd877f7631b2",
   "metadata": {},
   "source": [
    "### 5. What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e0485-4020-4210-b5b5-21d50dbdddb9",
   "metadata": {},
   "source": [
    "Data augmentation is a technique used to artificially increase the size of a dataset by creating new samples from the existing samples. This is done by applying various transformations to the existing samples, such as rotation, flipping, scaling, cropping, and adding noise. The goal of data augmentation is to improve the performance of machine learning models by introducing variations in the data and reducing overfitting. Data augmentation is particularly useful when the size of the dataset is small or when the data is imbalanced.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) is a popular data augmentation technique used to address imbalanced datasets. SMOTE works by creating synthetic samples of the minority class by interpolating between existing samples. The basic idea of SMOTE is to identify the minority class samples that are farthest away from their nearest neighbor in the minority class, and then generate new synthetic samples along the line segments joining those minority class samples. The number of new synthetic samples generated depends on the degree of imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48d9db6-a0cc-439a-adfb-71c1dfa57351",
   "metadata": {},
   "source": [
    "### 6. What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c76dbc-5a34-4ab8-acd7-a6c1d1e65272",
   "metadata": {},
   "source": [
    "Outliers are data points in a dataset that are significantly different from the other data points in the dataset. Outliers can be caused by various reasons, such as measurement errors, data entry errors, or rare events. Outliers can have a significant impact on the statistical analysis of the dataset and can lead to biased results.\n",
    "\n",
    "It is essential to handle outliers because they can have a significant impact on the analysis of the data, leading to erroneous conclusions. Outliers can skew the mean and standard deviation of the data, leading to incorrect estimates of the central tendency and variability of the data. Outliers can also affect the distribution of the data, leading to incorrect assumptions about the underlying distribution.\n",
    "\n",
    "Handling outliers is important to ensure that the statistical analysis of the data is accurate and reliable. Some common techniques for handling outliers include:\n",
    "\n",
    "Removing outliers: One approach to handling outliers is to remove them from the dataset. This can be done manually by identifying the outliers and removing them or automatically using statistical methods such as the z-score or the interquartile range (IQR).\n",
    "\n",
    "Transforming the data: Another approach to handling outliers is to transform the data. This can be done by applying a mathematical function such as the logarithm or square root to the data, which can reduce the impact of outliers.\n",
    "\n",
    "Replacing the outliers: Another approach to handling outliers is to replace them with a more representative value. This can be done by using statistical methods such as the mean or median of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031ff78d-98dc-4e2e-ba65-aacf47ac896b",
   "metadata": {},
   "source": [
    "### 7. You are working on a project that requires analyzing customer data. However, you notice that some ofthe data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ded079-4c33-406b-a38f-ca3be1d85d8a",
   "metadata": {},
   "source": [
    "Handling missing data is an important task in data analysis because missing data can lead to biased or inaccurate results. There are several techniques that can be used to handle missing data, depending on the nature of the data and the analysis being performed. Some of the commonly used techniques are:\n",
    "\n",
    "Deleting missing data: One approach to handling missing data is to delete the missing data from the dataset. This can be done by deleting the entire row or column containing the missing data. However, this approach can lead to loss of valuable information and reduce the sample size.\n",
    "\n",
    "Imputing missing data: Another approach to handling missing data is to impute the missing data by estimating the missing values based on the available data. Imputation methods include mean imputation, median imputation, regression imputation, and k-nearest neighbor imputation. However, imputation methods can introduce bias and affect the accuracy of the analysis.\n",
    "\n",
    "Using algorithms that can handle missing data: Some algorithms, such as decision trees and random forests, can handle missing data by splitting the data based on the available features. These algorithms can be used when the missing data is not too extensive.\n",
    "\n",
    "Creating a separate category for missing data: For categorical variables, a separate category can be created for missing data. This can be done by assigning a unique label to the missing data category.\n",
    "\n",
    "Collecting more data: Finally, collecting more data can help to reduce the impact of missing data by increasing the sample size and reducing the proportion of missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45132b3d-ac66-470c-a558-9cf8de5136f7",
   "metadata": {},
   "source": [
    "### 8. You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d7626-446b-427f-a741-12bc544c9e54",
   "metadata": {},
   "source": [
    "Determining if the missing data is missing at random (MAR) or missing not at random (MNAR) is important because it can affect the choice of method used to handle the missing data. If the missing data is MAR, then imputation methods can be used, whereas if the missing data is MNAR, then more advanced methods may be required.\n",
    "\n",
    "There are several strategies that can be used to determine if the missing data is MAR or MNAR:\n",
    "\n",
    "Visual inspection: One approach is to visually inspect the data for patterns or trends. This can be done by creating plots or graphs of the available data and examining if there are any relationships between the missing data and other variables in the dataset.\n",
    "\n",
    "Missingness tests: Another approach is to use statistical tests to determine if the missing data is missing at random. One common test is the Little's MCAR test, which tests if the missing data is completely random or not.\n",
    "\n",
    "Imputation and comparison: A third approach is to impute the missing data and compare the imputed values with the actual values. If the imputed values are similar to the actual values, then it is likely that the missing data is MAR. If the imputed values are significantly different from the actual values, then it is possible that the missing data is MNAR.\n",
    "\n",
    "Expert opinion: In some cases, expert knowledge may be required to determine if the missing data is MAR or MNAR. For example, a domain expert may be able to provide insight into why certain data is missing and whether it is likely to be MAR or MNAR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc55a63-f089-479e-9583-a556d7ca2166",
   "metadata": {},
   "source": [
    "### 9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3e19a8-c7cb-40a0-9b76-f77779bae633",
   "metadata": {},
   "source": [
    "When working with imbalanced datasets, where one class is significantly more prevalent than the other, the evaluation of the machine learning model's performance can be challenging. This is because a model that always predicts the majority class will have a high accuracy rate, but it may not be useful for predicting the minority class.\n",
    "\n",
    "To evaluate the performance of a machine learning model on an imbalanced dataset, some strategies that can be used are:\n",
    "\n",
    "Confusion matrix: A confusion matrix provides a detailed breakdown of the true positives, true negatives, false positives, and false negatives in a classification model. This can help to understand how well the model is performing for each class.\n",
    "\n",
    "Precision, Recall, and F1-Score: Precision measures how often the model is correct when it predicts the positive class, while recall measures how well the model identifies the positive class. F1-score is the harmonic mean of precision and recall. These metrics can provide a more detailed understanding of the model's performance for both classes.\n",
    "\n",
    "ROC and AUC: Receiver Operating Characteristic (ROC) curve is a graphical representation of the trade-off between the true positive rate and false positive rate for different threshold values. Area Under the Curve (AUC) is a metric that summarizes the ROC curve's performance. It can provide a measure of how well the model is performing overall, irrespective of the threshold value.\n",
    "\n",
    "Class weights: Assigning class weights can help to address the class imbalance problem by giving more weight to the minority class in the model's training. This can help to improve the model's performance for the minority class.\n",
    "\n",
    "Sampling methods: Up-sampling or down-sampling can be used to balance the dataset by increasing or decreasing the number of samples in the minority class. This can help to improve the model's performance for the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1974569-fa1b-4918-8ab6-69f826cebc4c",
   "metadata": {},
   "source": [
    "### 10.When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820b1b55-596d-4b47-99fe-738b5346e0f8",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset where one class is significantly more prevalent than the other, down-sampling the majority class can be used. Down-sampling involves randomly removing samples from the majority class to reduce its size to match the minority class. This can help to prevent the model from being biased towards the majority class and improve the overall performance for both classes.\n",
    "\n",
    "Here are some methods to down-sample the majority class:\n",
    "\n",
    "Random under-sampling: This involves randomly selecting a subset of samples from the majority class to match the size of the minority class. This method is simple to implement but can result in a loss of information.\n",
    "\n",
    "Cluster-based under-sampling: This involves clustering the majority class samples and then removing samples from each cluster to match the size of the minority class. This method can preserve the structure of the data better than random under-sampling.\n",
    "\n",
    "Tomek links: This involves identifying pairs of samples from different classes that are nearest neighbors of each other and then removing the majority class sample. This method can help to remove noisy samples and improve the decision boundary.\n",
    "\n",
    "Edited nearest neighbor: This involves identifying samples from the majority class that are misclassified by their nearest neighbors from the same class and removing them. This method can improve the decision boundary by removing noisy samples.\n",
    "\n",
    "Neighborhood cleaning rule: This involves identifying samples from the majority class that have a small number of minority class neighbors and removing them. This method can help to remove noisy samples and improve the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5872a4ca-3525-4544-8e10-1148e528b73b",
   "metadata": {},
   "source": [
    "### 11.You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3fcac9-1717-4b76-bf06-dbc0e7521f1b",
   "metadata": {},
   "source": [
    "When dealing with a dataset where the occurrence of a rare event is low, it is important to balance the dataset to avoid biased predictions. Up-sampling the minority class is a common method used to balance such datasets. The following are some techniques for up-sampling the minority class:\n",
    "\n",
    "Random over-sampling: This involves randomly duplicating the samples from the minority class to match the size of the majority class. This method can be effective but can lead to overfitting.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): This involves generating synthetic samples based on the existing minority class samples. SMOTE identifies nearest neighbors in the feature space and creates synthetic samples by interpolating between these neighbors. This method can help to avoid overfitting and improve the decision boundary.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): This is similar to SMOTE, but it generates synthetic samples based on the density of the minority class samples. The density of each sample is estimated using a kernel density estimator, and more synthetic samples are generated for samples with lower density. This method can be more effective than SMOTE when the distribution of the minority class is complex.\n",
    "\n",
    "Random Minority Over-sampling with Replacement (ROSE): This involves randomly selecting samples from the minority class and then generating synthetic samples using the selected samples. This method can help to avoid overfitting and improve the decision boundary.\n",
    "\n",
    "SMOTEBoost: This involves applying SMOTE iteratively during the boosting process to up-sample the minority class. This method can be effective when combined with boosting algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
