{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b4a542-a585-4314-bae2-17820f28ddf6",
   "metadata": {},
   "source": [
    "### 1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c48e6b1-06e3-4031-8d2a-7e5068ee054a",
   "metadata": {},
   "source": [
    "Min-max scaling is a data normalization technique that rescales features of a dataset to fit within a specific range. The goal of this technique is to bring all the features on the same scale so that the algorithm can easily learn from the data. It involves scaling the data in such a way that the minimum value of the feature is mapped to 0 and the maximum value of the feature is mapped to 1.\n",
    "\n",
    "The formula for min-max scaling is:\n",
    "\n",
    "X_scaled = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "where X is the original feature value, X_min is the minimum value of the feature, and X_max is the maximum value of the feature.\n",
    "\n",
    "For example, suppose we have a dataset of housing prices in which the minimum price is $100,000 and the maximum price is $1,000,000. We can use min-max scaling to rescale the data to fit within the range of 0 to 1. \n",
    "\n",
    "The min-max scaling formula will be applied to each value of the dataset as follows:\n",
    "\n",
    "X_scaled = (X - 100000) / (1000000 - 100000)\n",
    "\n",
    "#If a house costs $500,000, its scaled value will be:\n",
    "\n",
    "X_scaled = (500000 - 100000) / (1000000 - 100000) =0.44\n",
    "\n",
    "This means that the house price is 44.4% of the way from the minimum price to the maximum price. By applying this scaling technique to all the features in the dataset, we can ensure that they are all on the same scale, and the model can learn from them more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c3f8c-8374-4b2d-ae79-0c6cd3505009",
   "metadata": {},
   "source": [
    "### 2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73498149-cbab-49b4-a556-2b6e035d901f",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization, is another feature scaling method that rescales the data to ensure that all the features have the same scale. In this technique, each feature is divided by its magnitude or Euclidean norm, resulting in a unit vector of length 1. The goal of this technique is to make sure that each feature contributes equally to the distance computations in machine learning algorithms.\n",
    "\n",
    "The formula for unit vector scaling is:\n",
    "\n",
    "X_scaled = X / ||X||\n",
    "\n",
    "where X is the original feature vector, and ||X|| is the Euclidean norm of the vector, which is computed as:\n",
    "\n",
    "||X|| = sqrt(X_1^2 + X_2^2 + ... + X_n^2)\n",
    "\n",
    "where X_1, X_2, ..., X_n are the individual elements of the feature vector.\n",
    "\n",
    "For example, let's say we have a dataset of three features: age, income, and education level. We can apply unit vector scaling to the dataset as follows:\n",
    "\n",
    "\n",
    "X = [age, income, education]\n",
    "\n",
    "||X|| = sqrt(age^2 + income^2 + education^2)\n",
    "\n",
    "X_scaled = [age/||X||, income/||X||, education/||X||]\n",
    "\n",
    "Suppose we have a sample data point with age = 35, income = $50,000, and education = 16 years. We can apply the unit vector scaling to this data point as follows:\n",
    "\n",
    "||X|| = sqrt(35^2 + 50000^2 + 16^2) = 50035.994\n",
    "\n",
    "X_scaled = [35/50035.994, 50000/50035.994, 16/50035.994] = [0.0007, 0.9999, 0.0003]\n",
    "\n",
    "This means that the income feature has the highest magnitude and contributes the most to the distance computations, while the age and education level features contribute very little.\n",
    "\n",
    "The main difference between the unit vector technique and min-max scaling is that the former rescales the data to have a magnitude of 1, while the latter rescales the data to fit within a specific range. The unit vector technique is more appropriate when the scale of the feature values is not known in advance or when the magnitude of the features is important for the algorithm. Min-max scaling, on the other hand, is useful when the range of the feature values is known and needs to be standardized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341c6119-6cfe-4698-b50f-7a990221c321",
   "metadata": {},
   "source": [
    "### 3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd91d618-238f-4a23-9623-153c41eb5405",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction, which involves transforming high-dimensional datasets into a lower-dimensional space while retaining as much of the original information as possible. The goal of PCA is to identify the principal components of a dataset, which are linear combinations of the original features that capture the most variation in the data.\n",
    "\n",
    "PCA works by finding the directions of maximum variance in a dataset and projecting the data onto a lower-dimensional subspace defined by these directions. The first principal component is the direction of maximum variance, the second principal component is orthogonal to the first and has the second-highest variance, and so on. By retaining only the top k principal components, where k is smaller than the original number of features, we can reduce the dimensionality of the dataset.\n",
    "\n",
    "For example, let's say we have a dataset with three features: height, weight, and shoe size. We can use PCA to reduce the dimensionality of the dataset to two dimensions by identifying the two principal components that capture the most variation in the data.\n",
    "\n",
    "After standardizing the features (mean = 0, variance = 1), we can perform PCA as follows:\n",
    "\n",
    "Compute the covariance matrix of the standardized features.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Sort the eigenvectors in descending order of their corresponding eigenvalues.\n",
    "\n",
    "Select the top k eigenvectors with the highest eigenvalues to define the k principal components.\n",
    "\n",
    "Project the data onto the subspace defined by the selected principal components.\n",
    "\n",
    "Suppose the resulting eigenvectors are [0.7, 0.3, -0.6] and [0.1, 0.9, 0.4], and the corresponding eigenvalues are 1.8 and 0.6. We can select the first two eigenvectors to define the two principal components and project the data onto this subspace.\n",
    "\n",
    "The new feature vectors can be computed by multiplying the original standardized feature vectors by the eigenvector matrix:\n",
    "\n",
    "[0.7, 0.3] = [height, weight, shoe size] * [0.7, 0.3, -0.6]\n",
    "\n",
    "[0.1, 0.9] = [height, weight, shoe size] * [0.1, 0.9, 0.4]\n",
    "\n",
    "These two new features are the principal components that capture the most variation in the data, and the dimensionality of the dataset has been reduced from three to two. By dropping the shoe size feature, we have reduced the computational complexity of any algorithm trained on this dataset, without losing much information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1260d6-1bb7-4635-a644-a58a1facfef9",
   "metadata": {},
   "source": [
    "### 4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8c825e-9226-45fc-84ac-2e7d4f0f90de",
   "metadata": {},
   "source": [
    "PCA and Feature Extraction are closely related concepts, as PCA can be used as a feature extraction technique to extract a smaller set of meaningful features from a high-dimensional dataset. Feature extraction is the process of transforming raw input data into a reduced feature set that is more suitable for machine learning algorithms.\n",
    "\n",
    "In the context of PCA, feature extraction involves computing the principal components of a dataset and selecting a subset of these components as the new features. This process can be used to reduce the dimensionality of the dataset and eliminate irrelevant or redundant features.\n",
    "\n",
    "For example, let's say we have a dataset with 100 features, and we want to train a machine learning algorithm on this dataset. However, the high dimensionality of the dataset makes it difficult to train the algorithm, and some of the features may be irrelevant or redundant. We can use PCA as a feature extraction technique to identify the most important features and reduce the dimensionality of the dataset.\n",
    "\n",
    "After standardizing the features (mean = 0, variance = 1), we can perform PCA as described in the previous question to compute the principal components of the dataset. We can then select the top k principal components that capture the most variation in the data and use these components as the new features.\n",
    "\n",
    "For example, suppose we select the top 10 principal components, which capture 90% of the variance in the data. We can then use these 10 components as the new features and train the machine learning algorithm on this reduced feature set.\n",
    "\n",
    "The advantage of using PCA for feature extraction is that it can identify the most important features in the dataset and eliminate redundant or irrelevant features, reducing the computational complexity of the algorithm and improving its accuracy. It can also help with visualizing high-dimensional data and identifying patterns and trends in the data. However, it's important to note that the interpretability of the features may be reduced when using PCA for feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9882d09-8768-46df-b1ab-97bc19845bb5",
   "metadata": {},
   "source": [
    "### 5.You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f6f33e-861a-45bc-aca3-10030882789f",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, we can use Min-Max scaling to normalize the numerical features such as price, rating, and delivery time.\n",
    "\n",
    "Min-Max scaling is a common feature scaling technique that scales the features to a fixed range of values, typically between 0 and 1. This scaling technique preserves the relative relationships between the values in each feature and ensures that all features are on the same scale.\n",
    "\n",
    "To use Min-Max scaling to preprocess the food delivery dataset, we can follow these steps:\n",
    "\n",
    "Identify the numerical features that need to be scaled. In this case, we have identified price, rating, and delivery time.\n",
    "\n",
    "Compute the minimum and maximum values for each feature in the dataset.\n",
    "\n",
    "Use the formula (x - min) / (max - min) to scale each value in the feature to a range between 0 and 1.\n",
    "\n",
    "Replace the original values in the dataset with the scaled values.\n",
    "\n",
    "For example, let's say we have the following dataset with three features: price, rating, and delivery time.\n",
    "\n",
    "Price\tRating\tDelivery Time\n",
    "\n",
    "10\t4.5\t45\n",
    "\n",
    "15\t3.8\t30\n",
    "\n",
    "20\t4.2\t60\n",
    "\n",
    "12\t4.9\t50\n",
    "\n",
    "We can use Min-Max scaling to preprocess the dataset as follows:\n",
    "\n",
    "Identify the numerical features: price, rating, and delivery time.\n",
    "\n",
    "Compute the minimum and maximum values for each feature:\n",
    "\n",
    "Price: min = 10, max = 20\n",
    "\n",
    "Rating: min = 3.8, max = 4.9\n",
    "\n",
    "Delivery time: min = 30, max = 60\n",
    "\n",
    "Use the Min-Max scaling formula to scale each value in the feature to a range between 0 and 1:\n",
    "\n",
    "Scaled price = (price - 10) / (20 - 10)\n",
    "\n",
    "Scaled rating = (rating - 3.8) / (4.9 - 3.8)\n",
    "\n",
    "Scaled delivery time = (delivery time - 30) / (60 - 30)\n",
    "\n",
    "Replace the original values in the dataset with the scaled values:\n",
    "\n",
    "Scaled Price\tScaled Rating\tScaled Delivery Time\n",
    "\n",
    "0.00\t0.636\t0.375\n",
    "\n",
    "0.50\t0.091\t0.000\n",
    "\n",
    "1.00\t0.364\t1.000\n",
    "\n",
    "0.25\t1.000\t0.625\n",
    "\n",
    "By using Min-Max scaling to preprocess the data, we have normalized the numerical features to a range between 0 and 1, which can improve the performance of the recommendation system by ensuring that all features are on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e74013a-3adf-4efa-a66c-56d878a2839c",
   "metadata": {},
   "source": [
    "### 6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875eef2-a47f-4ded-bd6b-123d347d9dfe",
   "metadata": {},
   "source": [
    "To use PCA to reduce the dimensionality of the stock price dataset, we can follow these steps:\n",
    "\n",
    "Standardize the data: Before applying PCA, it is important to standardize the data to ensure that each feature is on the same scale. Standardization involves subtracting the mean from each data point and dividing by the standard deviation. This step is necessary because PCA is sensitive to the scale of the features.\n",
    "\n",
    "Compute the covariance matrix: PCA works by finding the directions of maximum variance in the data. The covariance matrix captures the relationships between the features and their variances.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: The eigenvectors of the covariance matrix represent the directions of maximum variance in the data. The eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "Select the principal components: The principal components are the eigenvectors that explain the most variance in the data. We can select a subset of the principal components to reduce the dimensionality of the dataset.\n",
    "\n",
    "Project the data onto the principal components: We can project the original data onto the principal components to obtain a new dataset with reduced dimensionality.\n",
    "\n",
    "For example, let's say we have a stock price dataset with 100 features. We can use PCA to reduce the dimensionality of the dataset as follows:\n",
    "\n",
    "Standardize the data: We subtract the mean from each data point and divide by the standard deviation to ensure that each feature is on the same scale.\n",
    "\n",
    "Compute the covariance matrix: We compute the covariance matrix to capture the relationships between the features and their variances.\n",
    "\n",
    "Compute the eigenvectors and eigenvalues: We compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "\n",
    "Select the principal components: We select the principal components that explain the most variance in the data. We can use a scree plot to visualize the amount of variance explained by each component and select a subset of the components that explains a sufficient amount of variance.\n",
    "\n",
    "Project the data onto the principal components: We project the original data onto the principal components to obtain a new dataset with reduced dimensionality. The new dataset will have fewer features than the original dataset, and each feature will be a linear combination of the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4d93a7-478a-4397-9e0f-a63e0ae7bbd9",
   "metadata": {},
   "source": [
    "### 7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2723b98d-4cc4-43a0-adb3-0d104376ab2a",
   "metadata": {},
   "source": [
    "To perform Min-Max scaling to transform the values to a range of -1 to 1, we can use the following formula:\n",
    "\n",
    "scaled_value = (value - min_value) / (max_value - min_value) * 2 - 1\n",
    "\n",
    "where min_value and max_value are the minimum and maximum values in the dataset, respectively.\n",
    "\n",
    "In this case, the minimum value is 1 and the maximum value is 20. Therefore:\n",
    "\n",
    "For the value 1: scaled_value = (1 - 1) / (20 - 1) * 2 - 1 = -1\n",
    "\n",
    "For the value 5: scaled_value = (5 - 1) / (20 - 1) * 2 - 1 = -0.6\n",
    "\n",
    "For the value 10: scaled_value = (10 - 1) / (20 - 1) * 2 - 1 = -0.2\n",
    "\n",
    "For the value 15: scaled_value = (15 - 1) / (20 - 1) * 2 - 1 = 0.2\n",
    "\n",
    "For the value 20: scaled_value = (20 - 1) / (20 - 1) * 2 - 1 = 1\n",
    "\n",
    "Therefore, the Min-Max scaled values for the dataset [1, 5, 10, 15, 20] with a range of -1 to 1 are [-1, -0.6, -0.2, 0.2, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14f7c1-2ced-4bf6-abcd-268508b6d3f3",
   "metadata": {},
   "source": [
    "### 8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bf9c9a-a256-47bd-864f-d8f7fdd158e7",
   "metadata": {},
   "source": [
    "Determining the number of principal components to retain in PCA depends on the desired level of explained variance and the trade-off between reducing the dimensionality of the dataset and preserving the information in the original features.\n",
    "\n",
    "To perform PCA on the given dataset, we would first standardize the features to have zero mean and unit variance. Then, we would compute the principal components and their corresponding eigenvalues, which represent the amount of variance explained by each component. We can then decide how many principal components to retain based on the cumulative explained variance and the eigenvalue threshold.\n",
    "\n",
    "Assuming that the dataset has a large number of observations and that the features are highly correlated, we might expect that a few principal components could capture most of the variance in the data. For example, we could aim to retain principal components that explain at least 90% of the total variance.\n",
    "\n",
    "However, since the given features are not specified, it is difficult to determine the exact number of principal components to retain without performing the PCA and analyzing the results. Therefore, we would need to perform PCA on the dataset and then evaluate the cumulative explained variance and the eigenvalue threshold to decide on the number of principal components to retain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
