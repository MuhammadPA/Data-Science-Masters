{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a5f0dd-a421-46cc-9d53-2246f11a6321",
   "metadata": {},
   "source": [
    "### 1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe44398a-f422-4387-923e-d71b5fb7cd3f",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique that belongs to the family of boosting algorithms. It is used for regression problems, where the goal is to predict a continuous numerical output based on a set of input features.\n",
    "\n",
    "In Gradient Boosting Regression, the model is built by iteratively combining multiple weak regression models, typically decision trees, to create a strong regression model. The key idea behind gradient boosting is to minimize the loss function of the model by optimizing the residuals or errors of the previous weak learners.\n",
    "\n",
    "Here's how Gradient Boosting Regression works:\n",
    "\n",
    "1. Initialization: The algorithm starts by initializing the model with a simple regression model, usually a constant value or the mean of the target variable.\n",
    "\n",
    "2. Training Weak Learners: In each iteration, a weak regression model, often a decision tree, is trained to fit the negative gradient (residuals) of the loss function of the current ensemble model. The weak learner aims to find the best split points based on the input features to minimize the residual errors.\n",
    "\n",
    "3. Gradient Calculation: The gradient of the loss function with respect to the predictions of the current ensemble model is computed. This gradient represents the direction and magnitude of the change needed to minimize the loss function.\n",
    "\n",
    "4. Updating the Ensemble Model: The predictions of the weak learner are scaled by a learning rate and added to the current ensemble model. The learning rate controls the contribution of each weak learner to the final prediction and helps prevent overfitting. The learning rate is typically a small value between 0 and 1.\n",
    "\n",
    "5. Iterative Process: Steps 2 to 4 are repeated for a specified number of iterations or until a predefined stopping criterion is met. Each iteration introduces a new weak learner that focuses on minimizing the remaining errors of the previous ensemble model.\n",
    "\n",
    "6. Final Prediction: The final prediction for a new instance is obtained by aggregating the predictions of all weak learners in the ensemble. Each weak learner's prediction is weighted by the learning rate, and the final prediction is the sum of these weighted predictions.\n",
    "\n",
    "Gradient Boosting Regression effectively combines the weak learners by iteratively fitting the negative gradient of the loss function, which allows subsequent weak learners to correct the mistakes made by the previous ones. This iterative process results in a strong regression model that achieves better predictive performance than individual weak learners.\n",
    "\n",
    "Popular implementations of Gradient Boosting Regression include the Gradient Boosting Machines (GBM) and XGBoost libraries. These implementations often provide additional features and optimizations to enhance the performance and scalability of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4296b612-0df2-41a8-ba3b-5e67dbdcf4e1",
   "metadata": {},
   "source": [
    "### 2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb2d3e1-d3c0-4d48-8667-8da96c74697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,Y = make_regression(n_samples=1000,n_features=1,n_informative=1, noise=20,random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9236bc4e-bb5d-40ba-8a4a-3532ecc0698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "383318f9-504d-45ad-bf04-d1bf73140d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of trees and learning rate\n",
    "n_estimators = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc6f13e-9254-4412-9cf8-dc7fe128c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ensemble predictions to the mean of the target variable\n",
    "import numpy as np\n",
    "ensemble_preds = np.full_like(ytrain, np.mean(ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c41ba53-2d4d-4823-af20-ce304c27bd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model using gradient boosting\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "stubs = []\n",
    "for i in range(n_estimators):\n",
    "    # Compute the residual between the current predictions and the true target values\n",
    "    residuals = ytrain - ensemble_preds\n",
    "    \n",
    "    # Fit a regression tree to the residuals\n",
    "    tree = DecisionTreeRegressor(max_depth=3)\n",
    "    tree.fit(xtrain, residuals)\n",
    "\n",
    "    stubs.append(tree)\n",
    "    \n",
    "    # Update the ensemble predictions with the current tree's predictions\n",
    "    ensemble_preds += learning_rate * tree.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "986544de-6db6-48ab-bd74-e583383e2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_pred = np.full_like(ytest, np.mean(ytrain))\n",
    "for i in range(n_estimators):\n",
    "    y_pred += learning_rate * stubs[i].predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a761ff-b78c-4fc0-8fb9-a45eac90ec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "mse = mean_squared_error(ytest, y_pred)\n",
    "r2 = r2_score(ytest, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363a01e5-099a-4688-b863-218d7ee231fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 822.8074571899081\n",
      "R-squared: 0.7836513313762161\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8990ad-9733-47e3-a37d-08f4880fbfed",
   "metadata": {},
   "source": [
    "### 3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432ffd9f-059f-42fe-9960-5565cb25693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate sample data\n",
    "X, Y = make_regression(n_samples=1000, n_features=5, n_informative=3, noise=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3c3357e-fb4c-4825-b691-b016beffa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(X,Y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bda41fd-4e8d-448e-87c9-2018d42f33e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f11edd43-f1dc-48ed-8b2f-d5c99d1b2f0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   scoring=&#x27;neg_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "                   param_distributions={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.5],\n",
       "                                        &#x27;max_depth&#x27;: [3, 5, 7],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200, 300]},\n",
       "                   scoring=&#x27;neg_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(), n_jobs=-1,\n",
       "                   param_distributions={'learning_rate': [0.01, 0.1, 0.5],\n",
       "                                        'max_depth': [3, 5, 7],\n",
       "                                        'n_estimators': [100, 200, 300]},\n",
       "                   scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a gradient boosting regressor object\n",
    "gbm = GradientBoostingRegressor()\n",
    "\n",
    "# Create a grid search object\n",
    "ran_search = RandomizedSearchCV(gbm,param_grid, \n",
    "                           scoring='neg_mean_squared_error',cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search object to the data\n",
    "ran_search.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27521fe5-cbb1-447e-831f-c02bd01e0726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.1}\n",
      "Best score:  -172.7208888892344\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters and their corresponding score\n",
    "print(\"Best hyperparameters: \", ran_search.best_params_)\n",
    "print(\"Best score: \", ran_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56a405fa-6411-4b11-8c1d-21d4857e656c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error:  165.12050773612955\n",
      "R-squared score:  0.9412037257192838\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the model on the test set\n",
    "y_pred = ran_search.predict(xtest)\n",
    "mse = mean_squared_error(ytest, y_pred)\n",
    "r2 = r2_score(ytest, y_pred)\n",
    "print(\"Mean squared error: \", mse)\n",
    "print(\"R-squared score: \", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011e4402-4a18-4372-8bd9-b02944ebd579",
   "metadata": {},
   "source": [
    "### 4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38212ac-b940-4432-874c-3ad0d86235f9",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a simple and relatively low-complexity model that performs slightly better than random guessing on a given learning task. It is often represented by a decision tree with a shallow depth, typically referred to as a decision stump.\n",
    "\n",
    "The concept of a weak learner is central to the boosting framework, where the idea is to sequentially combine multiple weak learners to create a strong learner. Each weak learner is trained to focus on the mistakes or residuals made by the ensemble of previously trained weak learners. By iteratively adding weak learners and updating the ensemble, the boosting algorithm improves the overall predictive performance.\n",
    "\n",
    "A weak learner is characterized by the following properties:\n",
    "\n",
    "1. Low Complexity: Weak learners are typically simple models with low complexity. For example, a decision stump is a decision tree with a single split and two leaf nodes. The simplicity of the weak learner ensures that it can only capture a limited set of relationships in the data.\n",
    "\n",
    "2. Slight Better-than-Random Performance: Although weak learners individually may not perform well, they are designed to perform slightly better than random guessing. They can consistently make predictions better than chance, albeit with limited accuracy.\n",
    "\n",
    "3. Focus on Mistakes: Weak learners are trained to focus on the mistakes or residuals made by the ensemble of previously trained weak learners. They learn to correct or improve upon the errors made by the previous weak learners in the boosting process.\n",
    "\n",
    "The strength of the ensemble model built using Gradient Boosting comes from the collective power of combining multiple weak learners. Each weak learner contributes its expertise in handling specific patterns or relationships in the data, and the boosting algorithm learns to weight their contributions appropriately.\n",
    "\n",
    "By iteratively adding weak learners and updating the ensemble based on the errors or residuals, Gradient Boosting can effectively reduce the overall prediction error and create a strong learner capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4cca93-8218-4525-a431-54ce79afb770",
   "metadata": {},
   "source": [
    "### 5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10116599-9a58-4194-a4d7-c267d0f0a20c",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be summarized as follows:\n",
    "\n",
    "1. Weak Learners Correct Mistakes: The algorithm starts with an initial weak learner (e.g., decision stump) that predicts the target variable. This initial model may not be accurate and is likely to make some mistakes in predictions.\n",
    "\n",
    "2. Focus on Mistakes: The algorithm then focuses on the instances where the initial model made mistakes. It calculates the residuals or errors by comparing the predicted values with the true values. The idea is to identify the instances where the model performed poorly and needs improvement.\n",
    "\n",
    "3. Sequential Model Building: The algorithm iteratively builds a sequence of weak learners to correct the mistakes made by the previous models. In each iteration, a new weak learner is added to the ensemble, and its objective is to predict the residuals or errors of the previous models accurately.\n",
    "\n",
    "4. Weighted Contribution: Each weak learner is assigned a weight or coefficient that determines its contribution to the final prediction. The weights are determined by how well the weak learner is able to correct the mistakes made by the previous models. The better a weak learner is at handling the residuals, the higher its weight in the final prediction.\n",
    "\n",
    "5. Gradual Error Reduction: By continuously adding weak learners and updating the ensemble based on the residuals, the algorithm gradually reduces the overall prediction error. Each new weak learner is focused on minimizing the errors made by the previous ensemble.\n",
    "\n",
    "6. Aggregation of Weak Learners: The final prediction is obtained by aggregating the predictions of all the weak learners in the ensemble, weighted by their respective coefficients. The ensemble combines the collective wisdom of multiple weak learners, each specializing in different aspects of the data.\n",
    "\n",
    "The intuition behind Gradient Boosting is that by iteratively correcting the mistakes made by the previous models and focusing on the instances that are difficult to predict, the algorithm can build a strong learner that performs well on the given learning task. It leverages the concept of gradient descent to optimize the ensemble by minimizing the loss function of the residuals. The overall result is a powerful ensemble model capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc8422a-5827-45c7-9e6b-f26056c9d47f",
   "metadata": {},
   "source": [
    "### 6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b1670-9310-4999-97fc-6b201fb55c58",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's a step-by-step explanation of how the algorithm constructs the ensemble:\n",
    "\n",
    "1. Initialize the Ensemble: The algorithm starts by initializing the ensemble with a simple model, typically a weak learner such as a decision stump or a small decision tree with a shallow depth. This initial model can be seen as the \"first approximation\" to the target variable.\n",
    "\n",
    "2. Calculate Residuals: The algorithm calculates the residuals or errors by comparing the predictions of the current ensemble with the true values of the target variable. The residuals represent the part of the target variable that the current ensemble failed to capture accurately.\n",
    "\n",
    "3. Train a Weak Learner: In each iteration, a new weak learner is trained to predict the residuals or errors made by the current ensemble. The weak learner is trained on the input features and the calculated residuals as the target variable. Its objective is to improve upon the errors made by the previous ensemble.\n",
    "\n",
    "4. Update the Ensemble: The new weak learner is added to the ensemble with a weight or coefficient that represents its contribution to the final prediction. The weight is determined based on how well the weak learner predicts the residuals. The better a weak learner is at handling the residuals, the higher its weight in the final prediction.\n",
    "\n",
    "5. Update the Predictions: The predictions of the ensemble are updated by adding the weighted predictions of the new weak learner. The ensemble becomes a combination of the previous weak learners and the newly added weak learner, with updated predictions for the target variable.\n",
    "\n",
    "6. Repeat the Process: Steps 2 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained to predict the residuals, and the ensemble is updated accordingly.\n",
    "\n",
    "7. Final Prediction: The final prediction is obtained by aggregating the predictions of all the weak learners in the ensemble, weighted by their respective coefficients. The ensemble combines the individual predictions of the weak learners to produce a more accurate and refined prediction for the target variable.\n",
    "\n",
    "By iteratively adding weak learners and updating the ensemble based on the residuals, the Gradient Boosting algorithm gradually reduces the overall prediction error and builds a strong ensemble model. The final ensemble leverages the collective knowledge of multiple weak learners, each specialized in different aspects of the data, to make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf09a4d1-000f-4a0e-beba-8946453aafd3",
   "metadata": {},
   "source": [
    "### 7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting  algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2e904d-289d-484f-8815-35049a05a4a8",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves several key steps. Here's an overview of the main steps:\n",
    "\n",
    "1. Define the Objective Function: The first step is to define an objective function that measures the performance or loss of the model on the training data. This function quantifies how well the model's predictions match the true values of the target variable. Typically, the objective function is based on a regression or classification metric such as mean squared error or cross-entropy loss.\n",
    "\n",
    "2. Initialize the Model: The algorithm starts by initializing the model with a constant value, which is often the mean or median of the target variable. This initial model serves as the \"zero approximation\" and sets the baseline for subsequent improvements.\n",
    "\n",
    "3. Calculate the Residuals: The residuals are calculated by subtracting the predictions of the current model from the true values of the target variable. The residuals represent the errors or discrepancies between the current model and the actual values.\n",
    "\n",
    "4. Train a Weak Learner: In each iteration, a weak learner is trained to predict the residuals made by the current model. The weak learner is typically a decision tree with a shallow depth or a simple linear model. Its objective is to capture the patterns or relationships in the residuals and generate predictions that correct the errors made by the current model.\n",
    "\n",
    "5. Update the Model: The predictions of the weak learner are scaled by a learning rate and added to the predictions of the current model. This update step adjusts the current model by a fraction of the predictions made by the weak learner. The learning rate controls the contribution of each weak learner to the final model and prevents overfitting.\n",
    "\n",
    "6. Repeat the Process: Steps 3 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met. In each iteration, a new weak learner is trained on the residuals, and the model is updated by incorporating the predictions of the weak learner. The process continues to refine the model and improve its predictions.\n",
    "\n",
    "7. Ensemble Prediction: The final prediction is obtained by summing the predictions of all the weak learners in the ensemble, along with the predictions made by the initial model. The ensemble combines the individual predictions to produce the overall prediction for the target variable.\n",
    "\n",
    "8. Gradient Descent Optimization: The updates made to the model in each iteration are based on gradient descent optimization. The objective is to minimize the loss function defined in the first step. The updates are performed in the direction of steepest descent by adjusting the parameters or weights of the weak learners.\n",
    "\n",
    "By following these steps, the Gradient Boosting algorithm iteratively constructs an ensemble of weak learners that collectively improve the model's performance. The mathematical intuition involves optimizing the objective function using gradient descent and leveraging the predictions of the weak learners to refine the model's predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
